{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/CAM_DS_301_Sentiment_analysis_and_text_classification_Activity_2_2_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 2.2.6 Sentiment analysis and text classification\n",
        "\n",
        "In this activity, you will build a sentiment analysis model using Python and a data set of customer reviews. You will preprocess the data and fine-tune, evaluate, and test the model.\n",
        "\n",
        "\n",
        "## Objective\n",
        "In this activity, you will download a data set from Hugging Face and conduct text classification on it. Your objective is to analyse how different parameter choices affect the performance of a sentiment classifier.\n",
        "\n",
        "You will complete this in your Notebook, where you will:\n",
        "\n",
        "- create and train sentiment classifier RNN models\n",
        "- evaluate model performance.\n",
        "\n",
        "\n",
        "\n",
        "## Assessment criteria\n",
        "\n",
        "By completing this activity, you will be able to provide evidence that you can:\n",
        "*   apply various text preprocessing techniques and representation methods to preprocess and analyse textual data.\n",
        "*   comprehend and implement different types of recurrent neural networks (RNNs) and understand their applications in NLP.\n",
        "*   build and fine-tune advanced NLP models for specific natural language processing tasks.\n",
        "\n",
        "\n",
        "## Activity guidance\n",
        "\n",
        "1. Install the necessary packages that will be useful in this activity\n",
        "2. Load the dataset sst5 from hugging face (https://huggingface.co/datasets/SetFit/sst5)\n",
        "\n",
        "3. Create dataframes of the train and train split\n",
        "4. Split the train dataframe into train and validation in the ratio of 8:2\n",
        "5. Preprocess the dataset, set the maximum size to 200, vocabulary size to 30000\n",
        "6. During tokenisation, mark out of vocabulary words as \"[OOV]\"\n",
        "7. Pad your sequences with special tokens\n",
        "8. Train a sentiment classifier on the dataset and compare different models for text classification\n",
        "9.Train for 5 epochs\n",
        "- Train with a vanilla RNN\n",
        "- Train with an LSTM\n",
        "- Is there any difference between a GRU and an LSTM?\n",
        "- Train with a bidirectional LSTM\n",
        "10. Comment on the performance of all the models\n"
      ],
      "metadata": {
        "id": "bJt5DBRG6FsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Start your activity here. Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "wUMKlbI5Cc3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vJ4plMp0wAra"
      },
      "outputs": [],
      "source": [
        "#In this activity, you will be required to download a dataset from huggingface and perfom the text classification on the the dataset\n",
        "#You will be required to study the impact of different different parameter choices on the classification perfomance of sentiment classifier\n",
        "\n",
        "\n",
        "#1. Install the necessary packages that will be useful in this  activity\n",
        "#2. Load the dataset sst5 from hugginface (https://huggingface.co/datasets/SetFit/sst5)\n",
        "#3.Create dataframes of the train and train split\n",
        "#4 Split the train dataframe into train and validation in the ratio of 8:2\n",
        "#5 Preprocess the dataset,  set the maximum size to 200, vocabulary size to 30000\n",
        "#6. During tokenization, mark out of vocabulary words as \"<OOV>\"\n",
        "#7 Pad your sequences with special tokens\n",
        "#8. Train a sentiment classifier on the dataset and compare different models for text classification\n",
        "# Train for 5 epochs\n",
        "#    - Train with a vanilla RNN\n",
        "#    - Train with an LSTM\n",
        "#    - Is there any difference between a GRU and an LSTM?\n",
        "#    - Train with a bidirectional LSTM\n",
        "# Comment on the perfomance of all the models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Install Necessary Packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "7fUAZyKeOJe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrqBL1cGN57x",
        "outputId": "150d0841-6c44-4cff-93eb-13fa2ade8454"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Load the Dataset from Hugging Face**"
      ],
      "metadata": {
        "id": "3jLCWeXEOObh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the sst5 dataset\n",
        "dataset = load_dataset(\"SetFit/sst5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U8uTfzAOA1l",
        "outputId": "e180c15c-f4d6-4934-9263-7570d0f76698"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][0])\n"
      ],
      "metadata": {
        "id": "qbBknuducn_a",
        "outputId": "95a5733f-5c6d-4f3d-c9e8-77474f7e2dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films', 'label': 4, 'label_text': 'very positive'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Create DataFrames for Train and Test Splits**"
      ],
      "metadata": {
        "id": "__c7kydEOWfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert train and test splits into DataFrames\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])"
      ],
      "metadata": {
        "id": "G2yWBAyhOUVe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of each DataFrame to confirm the conversion\n",
        "print(train_df.head())\n",
        "print(test_df.head())\n"
      ],
      "metadata": {
        "id": "cNhnqT0Cd0wn",
        "outputId": "51b08ac5-8ae5-491a-e0dc-00173009790d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label     label_text\n",
            "0  a stirring , funny and finally transporting re...      4  very positive\n",
            "1  apparently reassembled from the cutting-room f...      1       negative\n",
            "2  they presume their audience wo n't sit still f...      1       negative\n",
            "3  the entire movie is filled with deja vu moments .      2        neutral\n",
            "4  this is a visually stunning rumination on love...      3       positive\n",
            "                                                text  label     label_text\n",
            "0     no movement , no yuks , not much of anything .      1       negative\n",
            "1  a gob of drivel so sickly sweet , even the eag...      0  very negative\n",
            "2  ` how many more voyages can this limping but d...      2        neutral\n",
            "3  so relentlessly wholesome it made me want to s...      2        neutral\n",
            "4  gangs of new york is an unapologetic mess , wh...      0  very negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Split Train Data into Train and Validation (80:20)**"
      ],
      "metadata": {
        "id": "p__9P-bDOboV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "CLOzVU43PAXK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")"
      ],
      "metadata": {
        "id": "tHVcK7_aeCIJ",
        "outputId": "adf32937-810d-490b-d6d3-ffb4d95a4dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6835\n",
            "Validation set size: 1709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique label values in each split\n",
        "print(\"Unique labels in training set:\", train_df['label'].unique())\n",
        "print(\"Unique labels in validation set:\", val_df['label'].unique())\n",
        "print(\"Unique labels in test set:\", test_df['label'].unique())\n"
      ],
      "metadata": {
        "id": "AMhphkZslZxY",
        "outputId": "55716c18-4605-44a7-9f1a-af1c31443536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in training set: [3 2 1 4 0]\n",
            "Unique labels in validation set: [3 4 1 2 0]\n",
            "Unique labels in test set: [1 0 2 4 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "9Cib_HTXOdg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the maximum sequence length to 200 and the vocabulary size to 30,000.\n",
        "\n",
        "During tokenization, mark out-of-vocabulary (OOV) words as \"[OOV]\".\n",
        "\n",
        "1. Tokenize: Use a tokenizer, such as one from the Hugging Face library, with a maximum length and OOV handling.\n",
        "1. Padding: Add padding tokens to ensure each sequence is exactly 200 tokens long."
      ],
      "metadata": {
        "id": "GacjFl7wQwds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer with specific settings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "max_len = 200  # Maximum sequence length\n",
        "vocab_size = 30000  # Vocabulary size\n"
      ],
      "metadata": {
        "id": "m3ioQt6jZAAC",
        "outputId": "dfdcfde6-5dcf-4e57-c211-9c00b59f667a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the tokenizer with an out-of-vocabulary word\n",
        "sample_text = \"This is a sample sentence with an oovwordtest that should be replaced.\"\n",
        "\n",
        "# Tokenize the sample text\n",
        "tokens = tokenizer(sample_text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
        "print(\"Token IDs:\", tokens['input_ids'])\n",
        "print(\"Decoded Tokens:\", tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "\n"
      ],
      "metadata": {
        "id": "5xWlFKroeSUe",
        "outputId": "af2fff46-b8c2-43f6-b169-8e1f2e2f09f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [101, 2023, 2003, 1037, 7099, 6251, 2007, 2019, 1051, 4492, 18351, 22199, 2008, 2323, 2022, 2999, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded Tokens: ['[CLS]', 'this', 'is', 'a', 'sample', 'sentence', 'with', 'an', 'o', '##ov', '##word', '##test', 'that', 'should', 'be', 'replaced', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf # Import the TensorFlow library\n",
        "\n",
        "# Tokenize and preprocess the dataset\n",
        "def tokenize_and_encode(df):\n",
        "    return tokenizer(\n",
        "        df['text'].tolist(),\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "# Tokenize and prepare inputs for each dataset\n",
        "train_encodings = tokenize_and_encode(train_df)\n",
        "val_encodings = tokenize_and_encode(val_df)\n",
        "test_encodings = tokenize_and_encode(test_df)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = tf.convert_to_tensor(train_df['label'].values)\n",
        "val_labels = tf.convert_to_tensor(val_df['label'].values)\n",
        "test_labels = tf.convert_to_tensor(test_df['label'].values)\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask']\n",
        "}, train_labels)).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask']\n",
        "}, val_labels)).batch(32)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask']\n",
        "}, test_labels)).batch(32)\n"
      ],
      "metadata": {
        "id": "-7EYGpnsO_8p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print tokenized outputs for train_encodings\n",
        "print(\"Sample input IDs:\", train_encodings['input_ids'][0])\n",
        "print(\"Sample attention mask:\", train_encodings['attention_mask'][0])\n",
        "print(\"Length of input IDs:\", len(train_encodings['input_ids'][0]))\n"
      ],
      "metadata": {
        "id": "LdTFFnFVgBWd",
        "outputId": "c42e7091-c392-49fe-ced9-8a5475e618e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input IDs: tf.Tensor(\n",
            "[ 101 1010 1036 1036 2027 1005 2128 2041 2045  999 1005 1005  102    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0], shape=(200,), dtype=int32)\n",
            "Sample attention mask: tf.Tensor(\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(200,), dtype=int32)\n",
            "Length of input IDs: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch from the train dataset\n",
        "for batch in train_dataset.take(1):\n",
        "    inputs, labels = batch\n",
        "    print(\"Input IDs shape:\", inputs['input_ids'].shape)\n",
        "    print(\"Attention mask shape:\", inputs['attention_mask'].shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "id": "ejLHfGIqgIdQ",
        "outputId": "5ee3ea59-3e83-44c1-9b4f-3769e4983591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs shape: (32, 200)\n",
            "Attention mask shape: (32, 200)\n",
            "Labels shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Train a Sentiment Classifier and Compare Different Models**\n"
      ],
      "metadata": {
        "id": "KVi5rK_WOs5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train for 5 Epochs\n",
        "\n",
        "For each model, train on the preprocessed training data for 5 epochs.\n",
        "\n",
        "Below are the steps for setting up each model type:\n",
        "\n",
        "1. Vanilla RNN\n",
        "1. LSTM\n",
        "1. GRU (Compare its performance with LSTM)\n",
        "1. Bidirectional LSTM\n"
      ],
      "metadata": {
        "id": "I97wtjdJQT1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense\n",
        "\n",
        "# Model and dataset parameters\n",
        "vocab_size = 30000  # Defined earlier\n",
        "embedding_dim = 128  # Embedding dimension\n",
        "max_len = 200  # Maximum sequence length"
      ],
      "metadata": {
        "id": "PYxhlYXaO-vA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(rnn_type='VanillaRNN'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "\n",
        "    # Add the RNN layer\n",
        "    if rnn_type == 'VanillaRNN':\n",
        "        model.add(SimpleRNN(64))\n",
        "    elif rnn_type == 'LSTM':\n",
        "        model.add(LSTM(64))\n",
        "    elif rnn_type == 'GRU':\n",
        "        model.add(GRU(64))\n",
        "    elif rnn_type == 'BidirectionalLSTM':\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "    # Output layer for multi-class classification (5 classes)\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3HuOQoiulnhC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Training and Evaluation**"
      ],
      "metadata": {
        "id": "DLBnSIk0OwuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets with only input_ids and labels\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_encodings['input_ids'], train_labels)).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_encodings['input_ids'], val_labels)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_encodings['input_ids'], test_labels)).batch(32)\n"
      ],
      "metadata": {
        "id": "GFDTeyA0hFne"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_type in ['VanillaRNN', 'LSTM', 'GRU', 'BidirectionalLSTM']:\n",
        "    print(f\"\\nTraining {model_type} model...\")\n",
        "    model = create_model(rnn_type=model_type)\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=5\n",
        "    )\n",
        "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
        "    print(f\"{model_type} Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eFy-sJ8O9YE",
        "outputId": "820c1aa0-9d4c-4861-da3b-ce1ce4457803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training VanillaRNN model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 96ms/step - accuracy: 0.2530 - loss: 1.5869 - val_accuracy: 0.2721 - val_loss: 1.5676\n",
            "Epoch 2/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 99ms/step - accuracy: 0.2511 - loss: 1.5854 - val_accuracy: 0.2838 - val_loss: 1.5699\n",
            "Epoch 3/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 94ms/step - accuracy: 0.2595 - loss: 1.5822 - val_accuracy: 0.2768 - val_loss: 1.5856\n",
            "Epoch 4/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 98ms/step - accuracy: 0.2664 - loss: 1.5870 - val_accuracy: 0.2738 - val_loss: 1.5706\n",
            "Epoch 5/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.2601 - loss: 1.5912 - val_accuracy: 0.2756 - val_loss: 1.5703\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.2672 - loss: 1.5772\n",
            "VanillaRNN Validation Accuracy: 0.2756\n",
            "\n",
            "Training LSTM model...\n",
            "Epoch 1/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 167ms/step - accuracy: 0.2658 - loss: 1.5803 - val_accuracy: 0.2779 - val_loss: 1.5624\n",
            "Epoch 2/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 169ms/step - accuracy: 0.2706 - loss: 1.5745 - val_accuracy: 0.2779 - val_loss: 1.5623\n",
            "Epoch 3/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 168ms/step - accuracy: 0.2701 - loss: 1.5741 - val_accuracy: 0.2779 - val_loss: 1.5623\n",
            "Epoch 4/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 171ms/step - accuracy: 0.2697 - loss: 1.5739 - val_accuracy: 0.2779 - val_loss: 1.5622\n",
            "Epoch 5/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 172ms/step - accuracy: 0.2692 - loss: 1.5738 - val_accuracy: 0.2779 - val_loss: 1.5620\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.2689 - loss: 1.5721\n",
            "LSTM Validation Accuracy: 0.2779\n",
            "\n",
            "Training GRU model...\n",
            "Epoch 1/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 206ms/step - accuracy: 0.2554 - loss: 1.5793 - val_accuracy: 0.2779 - val_loss: 1.5630\n",
            "Epoch 2/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 200ms/step - accuracy: 0.2694 - loss: 1.5745 - val_accuracy: 0.2779 - val_loss: 1.5626\n",
            "Epoch 3/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 202ms/step - accuracy: 0.2704 - loss: 1.5742 - val_accuracy: 0.2779 - val_loss: 1.5625\n",
            "Epoch 4/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 203ms/step - accuracy: 0.2710 - loss: 1.5740 - val_accuracy: 0.2779 - val_loss: 1.5625\n",
            "Epoch 5/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 210ms/step - accuracy: 0.2698 - loss: 1.5738 - val_accuracy: 0.2779 - val_loss: 1.5623\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.2689 - loss: 1.5724\n",
            "GRU Validation Accuracy: 0.2779\n",
            "\n",
            "Training BidirectionalLSTM model...\n",
            "Epoch 1/5\n",
            "\u001b[1m214/214\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 308ms/step - accuracy: 0.2807 - loss: 1.5655 - val_accuracy: 0.3850 - val_loss: 1.4042\n",
            "Epoch 2/5\n",
            "\u001b[1m 19/214\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 263ms/step - accuracy: 0.3713 - loss: 1.3767"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 9: Visualize the Performance of All Models**\n"
      ],
      "metadata": {
        "id": "-F7GcwIoZb3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract model names and accuracies\n",
        "model_names = list(model_performance.keys())\n",
        "accuracies = list(model_performance.values())\n",
        "\n",
        "# Plot the performance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(model_names, accuracies, color=['blue', 'orange', 'green', 'purple'])\n",
        "plt.xlabel(\"Model Type\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Model Performance Comparison on Sentiment Analysis Task\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XXLphlv5Ze5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}