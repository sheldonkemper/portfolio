{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/CAM_DS_301_Sentiment_analysis_and_text_classification_Activity_2_2_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 2.2.6 Sentiment analysis and text classification\n",
        "\n",
        "In this activity, you will build a sentiment analysis model using Python and a data set of customer reviews. You will preprocess the data and fine-tune, evaluate, and test the model.\n",
        "\n",
        "\n",
        "## Objective\n",
        "In this activity, you will download a data set from Hugging Face and conduct text classification on it. Your objective is to analyse how different parameter choices affect the performance of a sentiment classifier.\n",
        "\n",
        "You will complete this in your Notebook, where you will:\n",
        "\n",
        "- create and train sentiment classifier RNN models\n",
        "- evaluate model performance.\n",
        "\n",
        "\n",
        "\n",
        "## Assessment criteria\n",
        "\n",
        "By completing this activity, you will be able to provide evidence that you can:\n",
        "*   apply various text preprocessing techniques and representation methods to preprocess and analyse textual data.\n",
        "*   comprehend and implement different types of recurrent neural networks (RNNs) and understand their applications in NLP.\n",
        "*   build and fine-tune advanced NLP models for specific natural language processing tasks.\n",
        "\n",
        "\n",
        "## Activity guidance\n",
        "\n",
        "1. Install the necessary packages that will be useful in this activity\n",
        "2. Load the dataset sst5 from hugging face (https://huggingface.co/datasets/SetFit/sst5)\n",
        "\n",
        "3. Create dataframes of the train and train split\n",
        "4. Split the train dataframe into train and validation in the ratio of 8:2\n",
        "5. Preprocess the dataset, set the maximum size to 200, vocabulary size to 30000\n",
        "6. During tokenisation, mark out of vocabulary words as \"[OOV]\"\n",
        "7. Pad your sequences with special tokens\n",
        "8. Train a sentiment classifier on the dataset and compare different models for text classification\n",
        "9.Train for 5 epochs\n",
        "- Train with a vanilla RNN\n",
        "- Train with an LSTM\n",
        "- Is there any difference between a GRU and an LSTM?\n",
        "- Train with a bidirectional LSTM\n",
        "10. Comment on the performance of all the models\n"
      ],
      "metadata": {
        "id": "bJt5DBRG6FsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Start your activity here. Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "wUMKlbI5Cc3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vJ4plMp0wAra"
      },
      "outputs": [],
      "source": [
        "#In this activity, you will be required to download a dataset from huggingface and perfom the text classification on the the dataset\n",
        "#You will be required to study the impact of different different parameter choices on the classification perfomance of sentiment classifier\n",
        "\n",
        "\n",
        "#1. Install the necessary packages that will be useful in this  activity\n",
        "#2. Load the dataset sst5 from hugginface (https://huggingface.co/datasets/SetFit/sst5)\n",
        "#3.Create dataframes of the train and train split\n",
        "#4 Split the train dataframe into train and validation in the ratio of 8:2\n",
        "#5 Preprocess the dataset,  set the maximum size to 200, vocabulary size to 30000\n",
        "#6. During tokenization, mark out of vocabulary words as \"<OOV>\"\n",
        "#7 Pad your sequences with special tokens\n",
        "#8. Train a sentiment classifier on the dataset and compare different models for text classification\n",
        "# Train for 5 epochs\n",
        "#    - Train with a vanilla RNN\n",
        "#    - Train with an LSTM\n",
        "#    - Is there any difference between a GRU and an LSTM?\n",
        "#    - Train with a bidirectional LSTM\n",
        "# Comment on the perfomance of all the models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Install Necessary Packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "7fUAZyKeOJe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tensorflow"
      ],
      "metadata": {
        "id": "mrqBL1cGN57x",
        "outputId": "f77b8775-e809-44a7-adc0-73069bcdcc7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Load the Dataset from Hugging Face**"
      ],
      "metadata": {
        "id": "3jLCWeXEOObh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the sst5 dataset\n",
        "dataset = load_dataset(\"SetFit/sst5\")\n"
      ],
      "metadata": {
        "id": "_U8uTfzAOA1l",
        "outputId": "0098be09-2311-44cd-a993-89bf07a8e014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Create DataFrames for Train and Test Splits**"
      ],
      "metadata": {
        "id": "__c7kydEOWfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert train and test splits into DataFrames\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])"
      ],
      "metadata": {
        "id": "G2yWBAyhOUVe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Split Train Data into Train and Validation (80:20)**"
      ],
      "metadata": {
        "id": "p__9P-bDOboV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "CLOzVU43PAXK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Preprocess the Dataset**"
      ],
      "metadata": {
        "id": "9Cib_HTXOdg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the maximum sequence length to 200 and the vocabulary size to 30,000.\n",
        "\n",
        "During tokenization, mark out-of-vocabulary (OOV) words as \"[OOV]\".\n",
        "\n",
        "1. Tokenize: Use a tokenizer, such as one from the Hugging Face library, with a maximum length and OOV handling.\n",
        "1. Padding: Add padding tokens to ensure each sequence is exactly 200 tokens long."
      ],
      "metadata": {
        "id": "GacjFl7wQwds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\" , unk_token=\"[OOV]\")\n",
        "max_len = 200\n",
        "\n",
        "def preprocess(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "train_encodings = preprocess(train_df['text'].tolist())\n",
        "val_encodings = preprocess(val_df['text'].tolist())\n",
        "test_encodings = preprocess(test_df['text'].tolist())\n"
      ],
      "metadata": {
        "id": "-7EYGpnsO_8p",
        "outputId": "197f89a3-8ebf-405d-b88e-bc0aaed2f2f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Train a Sentiment Classifier and Compare Different Models**\n"
      ],
      "metadata": {
        "id": "KVi5rK_WOs5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train for 5 Epochs\n",
        "\n",
        "For each model, train on the preprocessed training data for 5 epochs.\n",
        "\n",
        "Below are the steps for setting up each model type:\n",
        "\n",
        "1. Vanilla RNN\n",
        "1. LSTM\n",
        "1. GRU (Compare its performance with LSTM)\n",
        "1. Bidirectional LSTM\n"
      ],
      "metadata": {
        "id": "I97wtjdJQT1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense\n",
        "\n",
        "vocab_size = 30000\n",
        "embedding_dim = 128\n",
        "\n",
        "def create_model(rnn_type='VanillaRNN'):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "\n",
        "    if rnn_type == 'VanillaRNN':\n",
        "        model.add(SimpleRNN(64))\n",
        "    elif rnn_type == 'LSTM':\n",
        "        model.add(LSTM(64))\n",
        "    elif rnn_type == 'GRU':\n",
        "        model.add(GRU(64))\n",
        "    elif rnn_type == 'BidirectionalLSTM':\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "PYxhlYXaO-vA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Training and Evaluation**"
      ],
      "metadata": {
        "id": "DLBnSIk0OwuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_labels)).batch(32)\n",
        "\n",
        "# Create and train each model\n",
        "for model_type in ['VanillaRNN', 'LSTM', 'GRU', 'BidirectionalLSTM']:\n",
        "    print(f\"Training {model_type} model...\")\n",
        "    model = create_model(rnn_type=model_type)\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=5\n",
        "    )\n",
        "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
        "    print(f\"{model_type} Validation Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "2eFy-sJ8O9YE",
        "outputId": "2c1ac832-7d26-4f2f-b4ca-121572e49bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_inputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d3df42ef5efc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to TensorFlow dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create and train each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
          ]
        }
      ]
    }
  ]
}