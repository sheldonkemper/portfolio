{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LIrag9UBW4Gx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/CAM_DS_C201_Activity_3_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 3.2.3 Experimenting with hyperparameter tuning\n",
        "\n",
        "## Scenario\n",
        "Hopkins et al. (1999) created the Spambase data set donated to the UCI Machine Learning Repository. The data set contains 4,601 emails marked as spam or non-spam by a postmaster or individuals. Fifty-seven features aid in classifying emails as spam (e.g. word frequencies and email characteristics). The Spambase data set is used for developing and benchmarking spam detection models, providing a base for analysing the effectiveness of various machine learning techniques in distinguishing between spam and legitimate emails.\n",
        "\n",
        "As a data professional, you were tasked by your company to develop a neural network with TensorFlow that can classify emails as spam or non-spam. You were tasked to develop a model based on the Spambase data set.\n",
        "\n",
        "\n",
        "\n",
        "## Objective\n",
        "In this portfolio activity, you’ll continue to work with the model you created in Activity 3.1.5 by applying model tuning and grid search to classify emails as spam or non-spam.\n",
        "\n",
        "You will complete the activity in your Notebook, where you’ll:\n",
        "- add an extra four layers to the model you created previously\n",
        "- create a new model pipeline\n",
        "- employ different batch sizes and epochs to evaluate the impact on the accuracy\n",
        "- present your insights based on the performance of the model.\n",
        "\n",
        "\n",
        "## Assessment criteria\n",
        "By completing this activity, you will be able to provide evidence that you can critically select appropriate strategies to demonstrate expertise in model tuning techniques.\n",
        "\n",
        "\n",
        "## Activity guidance\n",
        "1. Continue to work on the model you created in **Activity 3.1.5**.\n",
        "2. Add 4 hidden layers with the ReLU activation and 16 neurons for the fourth layer.\n",
        "3. Compile the model with `binary_crossentropy` as loss, Adam optimiser, and print the accuracy of the model.\n",
        "4. Train and evaluate the model again.\n",
        "5. Jot down whether the final evaluation changed? Was there any improvement in the model? If not, train and evaluate again. Does the final evaluation change? Does it improve?\n",
        "6. Create a vector of different `batch_sizes=np.array([16, 32, 64])` and `loop` through it, retraining the model each time, and print the performances. Use the same model and number of epochs.\n",
        "7. Create a vector of different `epochs=np.array([10, 20, 30])` and `loop` through it, retraining the model each time and using the batch size. Jot down which model gave you the highest accuracy."
      ],
      "metadata": {
        "id": "LIrag9UBW4Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Start your activity here. Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "N-dwQn8pfPDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "RGFfjIIbrcSM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your activity here:\n",
        "\n",
        "# URL to import data set from GitHub.\n",
        "url = 'https://raw.githubusercontent.com/fourthrevlxd/cam_dsb/main/spamdata.csv'"
      ],
      "metadata": {
        "id": "b-rDvxDllBhU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(url, header = None)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "chK3ffTypsdo",
        "outputId": "63dd9d4c-1397-4927-b27b-bfd6da4d4a88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
              "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
              "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
              "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
              "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
              "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
              "\n",
              "      49   50     51     52     53     54   55    56  57  \n",
              "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
              "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
              "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
              "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
              "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
              "\n",
              "[5 rows x 58 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f360ec94-28f4-44d0-8324-da8bd6830c87\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 58 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f360ec94-28f4-44d0-8324-da8bd6830c87')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f360ec94-28f4-44d0-8324-da8bd6830c87 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f360ec94-28f4-44d0-8324-da8bd6830c87');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-569d7a00-50a2-44e7-b69c-e85c47a41daf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-569d7a00-50a2-44e7-b69c-e85c47a41daf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-569d7a00-50a2-44e7-b69c-e85c47a41daf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]"
      ],
      "metadata": {
        "id": "cqYWpHKPlBax"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X ,y, test_size=0.2, random_state = 42)\n",
        "# Further split the training set into train and validation (90% train, 10% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "sMgjjpiylBUj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "# Fit the scaler on the training data and transform it\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "sRqAQavCp4aR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =  tf.keras.Sequential()\n",
        "number_neurons_1 = 64\n",
        "number_neurons_2 = 32\n",
        "\n",
        "#hiden layer.\n",
        "model.add(tf.keras.layers.Dense(number_neurons_1,activation = 'relu', input_shape = (X_train.shape[1],)))\n",
        "model.add(tf.keras.layers.Dense(number_neurons_2,activation = 'relu'))\n",
        "\n",
        "# Adding the new hidden layers\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "\n",
        "#output layer\n",
        "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46JGtFu1p8By",
        "outputId": "8fe712f4-393f-4c44-bc4d-d9263e11414e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using TensorFlow's alias\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yxae237Qp_Vk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH7ImuLYqCfg",
        "outputId": "be79c08d-acdc-46ac-d557-c54743128fd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5665 - loss: 0.6802 - val_accuracy: 0.8995 - val_loss: 0.4260\n",
            "Epoch 2/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.3396 - val_accuracy: 0.9266 - val_loss: 0.2186\n",
            "Epoch 3/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9261 - loss: 0.2012 - val_accuracy: 0.9348 - val_loss: 0.2020\n",
            "Epoch 4/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9371 - loss: 0.1849 - val_accuracy: 0.9375 - val_loss: 0.1957\n",
            "Epoch 5/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9508 - loss: 0.1560 - val_accuracy: 0.9375 - val_loss: 0.1919\n",
            "Epoch 6/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.1386 - val_accuracy: 0.9348 - val_loss: 0.1846\n",
            "Epoch 7/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9536 - loss: 0.1389 - val_accuracy: 0.9429 - val_loss: 0.1884\n",
            "Epoch 8/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9526 - loss: 0.1277 - val_accuracy: 0.9375 - val_loss: 0.1834\n",
            "Epoch 9/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1158 - val_accuracy: 0.9348 - val_loss: 0.2036\n",
            "Epoch 10/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9648 - loss: 0.1046 - val_accuracy: 0.9375 - val_loss: 0.1988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "# Print the loss and accuracy\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU6XQoDVqFql",
        "outputId": "6af2cc1e-44c1-46e5-8d5d-aa5382c7bef1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9364 - loss: 0.1919 \n",
            "Test Loss: 0.1658, Test Accuracy: 0.9435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **My Evaluation of the Model's Performance**\n",
        "\n",
        "After training the model with additional hidden layers, I wanted to see if the increased complexity would improve the accuracy of my spam detection model. Here’s a breakdown of how things went.\n",
        "\n",
        "#### **Previous Model Performance**:\n",
        "- **Test Accuracy**: 94.68%\n",
        "- **Test Loss**: 0.1542\n",
        "- **Training Accuracy**: 93.79%\n",
        "- **Training Loss**: 0.1612\n",
        "\n",
        "#### **New Model (with 4 additional layers) Performance**:\n",
        "- **Test Accuracy**: 94.35%\n",
        "- **Test Loss**: 0.1658\n",
        "- **Training Accuracy**: 93.64%\n",
        "- **Training Loss**: 0.1919\n",
        "\n",
        "### **What I Observed**:\n",
        "\n",
        "1. **Slight Drop in Accuracy**:  \n",
        "   The first thing I noticed was that the test accuracy dropped a little after adding the 4 extra layers—from **94.68%** to **94.35%**. While the difference isn't huge, it shows that the extra layers didn’t provide the expected boost in performance. In fact, the simpler model did a better job of predicting spam emails.\n",
        "\n",
        "2. **Increase in Loss**:  \n",
        "   The test loss also increased slightly from **0.1542** to **0.1658**. This is another sign that the additional layers didn’t help the model generalise better. Ideally, I want the loss to decrease, meaning the model is making fewer errors, but in this case, adding complexity introduced more errors.\n",
        "\n",
        "3. **Training Performance**:  \n",
        "   Interestingly, the training accuracy also dropped, and the training loss increased. The **training accuracy** went down slightly to **93.64%**, and the **training loss** rose to **0.1919**. This suggests that the model struggled more during training with the added complexity, which might indicate the model was overfitting.\n",
        "\n",
        "### **My Take on the Results**:\n",
        "\n",
        "Adding the extra layers didn’t improve the model’s performance—it slightly worsened it. The original, simpler model did a better job of both generalising to unseen data and performing well on the training set.\n",
        "\n",
        "I expected that adding more layers would allow the model to learn more complex patterns in the data, but in this case, it seems that the additional layers added unnecessary complexity. This might have led to overfitting, where the model was learning the training data too well but couldn’t generalise as effectively to new data."
      ],
      "metadata": {
        "id": "7Sn4MGNHlSu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_sizes = np.array([16, 32, 64])\n",
        "\n",
        "# Loop through different batch sizes\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_valid, y_valid))\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Batch Size: {batch_size}, Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAfNQrtxt0PK",
        "outputId": "a6568f4b-20ae-4e03-fe4c-42a76e3ee6d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with batch size: 16\n",
            "Epoch 1/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9629 - loss: 0.1215 - val_accuracy: 0.9348 - val_loss: 0.1890\n",
            "Epoch 2/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1186 - val_accuracy: 0.9402 - val_loss: 0.2007\n",
            "Epoch 3/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9662 - loss: 0.1030 - val_accuracy: 0.9348 - val_loss: 0.2290\n",
            "Epoch 4/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.0916 - val_accuracy: 0.9293 - val_loss: 0.2307\n",
            "Epoch 5/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.0928 - val_accuracy: 0.9402 - val_loss: 0.2312\n",
            "Epoch 6/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9743 - loss: 0.0673 - val_accuracy: 0.9375 - val_loss: 0.2392\n",
            "Epoch 7/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9685 - loss: 0.0878 - val_accuracy: 0.9375 - val_loss: 0.2525\n",
            "Epoch 8/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.0612 - val_accuracy: 0.9402 - val_loss: 0.2508\n",
            "Epoch 9/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0589 - val_accuracy: 0.9429 - val_loss: 0.2612\n",
            "Epoch 10/10\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9740 - loss: 0.0822 - val_accuracy: 0.9348 - val_loss: 0.2743\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9251 - loss: 0.3145 \n",
            "Batch Size: 16, Test Accuracy: 0.9349, Test Loss: 0.2407\n",
            "\n",
            "Training with batch size: 32\n",
            "Epoch 1/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9886 - loss: 0.0442 - val_accuracy: 0.9429 - val_loss: 0.2667\n",
            "Epoch 2/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9891 - loss: 0.0355 - val_accuracy: 0.9348 - val_loss: 0.2870\n",
            "Epoch 3/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0444 - val_accuracy: 0.9375 - val_loss: 0.2868\n",
            "Epoch 4/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0344 - val_accuracy: 0.9484 - val_loss: 0.2918\n",
            "Epoch 5/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0375 - val_accuracy: 0.9375 - val_loss: 0.3121\n",
            "Epoch 6/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0411 - val_accuracy: 0.9375 - val_loss: 0.3093\n",
            "Epoch 7/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0412 - val_accuracy: 0.9293 - val_loss: 0.3440\n",
            "Epoch 8/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0380 - val_accuracy: 0.9321 - val_loss: 0.3365\n",
            "Epoch 9/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0330 - val_accuracy: 0.9402 - val_loss: 0.3301\n",
            "Epoch 10/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0270 - val_accuracy: 0.9375 - val_loss: 0.3425\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9300 - loss: 0.4182 \n",
            "Batch Size: 32, Test Accuracy: 0.9435, Test Loss: 0.2769\n",
            "\n",
            "Training with batch size: 64\n",
            "Epoch 1/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9906 - loss: 0.0278 - val_accuracy: 0.9348 - val_loss: 0.3513\n",
            "Epoch 2/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0274 - val_accuracy: 0.9375 - val_loss: 0.3681\n",
            "Epoch 3/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0206 - val_accuracy: 0.9293 - val_loss: 0.3706\n",
            "Epoch 4/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0276 - val_accuracy: 0.9321 - val_loss: 0.3803\n",
            "Epoch 5/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9945 - loss: 0.0194 - val_accuracy: 0.9348 - val_loss: 0.3766\n",
            "Epoch 6/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0285 - val_accuracy: 0.9321 - val_loss: 0.3844\n",
            "Epoch 7/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0244 - val_accuracy: 0.9321 - val_loss: 0.4097\n",
            "Epoch 8/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0234 - val_accuracy: 0.9321 - val_loss: 0.3983\n",
            "Epoch 9/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0282 - val_accuracy: 0.9293 - val_loss: 0.3991\n",
            "Epoch 10/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9922 - loss: 0.0235 - val_accuracy: 0.9293 - val_loss: 0.4027\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9342 - loss: 0.4898 \n",
            "Batch Size: 64, Test Accuracy: 0.9501, Test Loss: 0.3108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of Results**:\n",
        "- **Batch Size 16**: Test Accuracy = 93.49%, Test Loss = 0.2407\n",
        "- **Batch Size 32**: Test Accuracy = 94.35%, Test Loss = 0.2769\n",
        "- **Batch Size 64**: Test Accuracy = 95.01%, Test Loss = 0.3108\n",
        "\n",
        "### **Final Analysis**:\n",
        "1. **Best Accuracy**: The model performed best in terms of accuracy with a batch size of **64** (**95.01% test accuracy**), but the higher test loss indicates some trade-offs between accuracy and generalisation. This suggests that the model might have focused on fitting the data well but still made some errors.\n",
        "\n",
        "2. **Lower Test Loss**: The batch size of **16** gave the **lowest test loss** of **0.2407**, meaning the model made fewer mistakes but did not achieve the highest accuracy.\n",
        "\n",
        "3. **Overfitting**: Across all batch sizes, there was some level of overfitting. The training accuracy was consistently higher than both validation and test accuracy, indicating that the model could be memorising the training data.\n",
        "\n",
        "### **Recommendations**:\n",
        "- **Batch Size 64** offers the best accuracy, but if loss minimisation is more important, then **Batch Size 16** may be a better choice.\n",
        "- I could try **adding regularisation** (such as dropout) to mitigate overfitting.\n",
        "- It might also be worth experimenting with more epochs or slightly adjusting the learning rate to further reduce overfitting and improve generalisation.\n",
        "\n",
        "Would you like me to explore hyperparameter tuning or anything else further?"
      ],
      "metadata": {
        "id": "WD-cnxoWuxB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.array([10, 20, 30])\n",
        "best_batch_size = 64  # Use the batch size that worked best previously\n",
        "\n",
        "# Loop through different epochs\n",
        "for epoch in epochs:\n",
        "    print(f\"\\nTraining with epochs: {epoch}\")\n",
        "    model.fit(X_train, y_train, epochs=epoch, batch_size=best_batch_size, validation_data=(X_valid, y_valid))\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Epochs: {epoch}, Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzaLO6CCt9VS",
        "outputId": "63ea41b7-365e-4b20-f696-437eb28daa38"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with epochs: 10\n",
            "Epoch 1/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0281 - val_accuracy: 0.9321 - val_loss: 0.4125\n",
            "Epoch 2/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0252 - val_accuracy: 0.9266 - val_loss: 0.4338\n",
            "Epoch 3/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0259 - val_accuracy: 0.9266 - val_loss: 0.4351\n",
            "Epoch 4/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0240 - val_accuracy: 0.9293 - val_loss: 0.4277\n",
            "Epoch 5/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9916 - loss: 0.0244 - val_accuracy: 0.9239 - val_loss: 0.4575\n",
            "Epoch 6/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0243 - val_accuracy: 0.9293 - val_loss: 0.4371\n",
            "Epoch 7/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0243 - val_accuracy: 0.9185 - val_loss: 0.4596\n",
            "Epoch 8/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 0.0240 - val_accuracy: 0.9293 - val_loss: 0.4600\n",
            "Epoch 9/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0249 - val_accuracy: 0.9239 - val_loss: 0.4684\n",
            "Epoch 10/10\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0188 - val_accuracy: 0.9293 - val_loss: 0.4782\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9306 - loss: 0.5726 \n",
            "Epochs: 10, Test Accuracy: 0.9457, Test Loss: 0.3544\n",
            "\n",
            "Training with epochs: 20\n",
            "Epoch 1/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9931 - loss: 0.0215 - val_accuracy: 0.9266 - val_loss: 0.4683\n",
            "Epoch 2/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0207 - val_accuracy: 0.9185 - val_loss: 0.4827\n",
            "Epoch 3/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0249 - val_accuracy: 0.9185 - val_loss: 0.5642\n",
            "Epoch 4/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.0366 - val_accuracy: 0.9212 - val_loss: 0.5355\n",
            "Epoch 5/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0194 - val_accuracy: 0.9185 - val_loss: 0.5235\n",
            "Epoch 6/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0198 - val_accuracy: 0.9239 - val_loss: 0.5096\n",
            "Epoch 7/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0166 - val_accuracy: 0.9239 - val_loss: 0.4930\n",
            "Epoch 8/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0281 - val_accuracy: 0.9239 - val_loss: 0.5045\n",
            "Epoch 9/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0377 - val_accuracy: 0.9185 - val_loss: 0.5300\n",
            "Epoch 10/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0206 - val_accuracy: 0.9185 - val_loss: 0.5488\n",
            "Epoch 11/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0201 - val_accuracy: 0.9239 - val_loss: 0.5504\n",
            "Epoch 12/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9946 - loss: 0.0179 - val_accuracy: 0.9239 - val_loss: 0.5228\n",
            "Epoch 13/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9947 - loss: 0.0163 - val_accuracy: 0.9239 - val_loss: 0.6026\n",
            "Epoch 14/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0206 - val_accuracy: 0.9158 - val_loss: 0.5894\n",
            "Epoch 15/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9930 - loss: 0.0183 - val_accuracy: 0.9130 - val_loss: 0.6049\n",
            "Epoch 16/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9942 - loss: 0.0197 - val_accuracy: 0.9212 - val_loss: 0.6102\n",
            "Epoch 17/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9950 - loss: 0.0176 - val_accuracy: 0.9158 - val_loss: 0.6143\n",
            "Epoch 18/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0138 - val_accuracy: 0.9158 - val_loss: 0.6048\n",
            "Epoch 19/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9956 - loss: 0.0142 - val_accuracy: 0.9158 - val_loss: 0.6283\n",
            "Epoch 20/20\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0154 - val_accuracy: 0.9185 - val_loss: 0.6090\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9319 - loss: 0.5830 \n",
            "Epochs: 20, Test Accuracy: 0.9479, Test Loss: 0.3684\n",
            "\n",
            "Training with epochs: 30\n",
            "Epoch 1/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0173 - val_accuracy: 0.9185 - val_loss: 0.5550\n",
            "Epoch 2/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9953 - loss: 0.0171 - val_accuracy: 0.9158 - val_loss: 0.5525\n",
            "Epoch 3/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0173 - val_accuracy: 0.9212 - val_loss: 0.5989\n",
            "Epoch 4/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0138 - val_accuracy: 0.9212 - val_loss: 0.5804\n",
            "Epoch 5/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0182 - val_accuracy: 0.9239 - val_loss: 0.5149\n",
            "Epoch 6/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0149 - val_accuracy: 0.9239 - val_loss: 0.5756\n",
            "Epoch 7/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9933 - loss: 0.0219 - val_accuracy: 0.9158 - val_loss: 0.6030\n",
            "Epoch 8/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9921 - loss: 0.0224 - val_accuracy: 0.9185 - val_loss: 0.5973\n",
            "Epoch 9/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0179 - val_accuracy: 0.9212 - val_loss: 0.5740\n",
            "Epoch 10/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0255 - val_accuracy: 0.9130 - val_loss: 0.4971\n",
            "Epoch 11/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0325 - val_accuracy: 0.9022 - val_loss: 0.5849\n",
            "Epoch 12/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0291 - val_accuracy: 0.9212 - val_loss: 0.4730\n",
            "Epoch 13/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.0158 - val_accuracy: 0.9212 - val_loss: 0.5080\n",
            "Epoch 14/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0140 - val_accuracy: 0.9130 - val_loss: 0.5566\n",
            "Epoch 15/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0181 - val_accuracy: 0.9185 - val_loss: 0.5443\n",
            "Epoch 16/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9940 - loss: 0.0171 - val_accuracy: 0.9185 - val_loss: 0.5541\n",
            "Epoch 17/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0171 - val_accuracy: 0.9239 - val_loss: 0.5584\n",
            "Epoch 18/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9960 - loss: 0.0148 - val_accuracy: 0.9130 - val_loss: 0.5778\n",
            "Epoch 19/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0105 - val_accuracy: 0.9158 - val_loss: 0.6206\n",
            "Epoch 20/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0176 - val_accuracy: 0.9158 - val_loss: 0.5869\n",
            "Epoch 21/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0170 - val_accuracy: 0.9158 - val_loss: 0.6213\n",
            "Epoch 22/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9952 - loss: 0.0156 - val_accuracy: 0.9239 - val_loss: 0.5687\n",
            "Epoch 23/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0121 - val_accuracy: 0.9239 - val_loss: 0.5735\n",
            "Epoch 24/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9939 - loss: 0.0170 - val_accuracy: 0.9239 - val_loss: 0.5815\n",
            "Epoch 25/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.0136 - val_accuracy: 0.9239 - val_loss: 0.5725\n",
            "Epoch 26/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0222 - val_accuracy: 0.9158 - val_loss: 0.6021\n",
            "Epoch 27/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9962 - loss: 0.0108 - val_accuracy: 0.9239 - val_loss: 0.5730\n",
            "Epoch 28/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9953 - loss: 0.0126 - val_accuracy: 0.9158 - val_loss: 0.5797\n",
            "Epoch 29/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0113 - val_accuracy: 0.9266 - val_loss: 0.5719\n",
            "Epoch 30/30\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9967 - loss: 0.0107 - val_accuracy: 0.9293 - val_loss: 0.5753\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9363 - loss: 0.5892 \n",
            "Epochs: 30, Test Accuracy: 0.9501, Test Loss: 0.3759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis by Epoch**:\n",
        "\n",
        "1. **10 Epochs**:\n",
        "   - **Strength**: Reasonably good accuracy with minimal overfitting.\n",
        "   - **Weakness**: Lower test accuracy compared to models with more epochs, though this could be acceptable if avoiding overfitting is a priority.\n",
        "\n",
        "2. **20 Epochs**:\n",
        "   - **Strength**: Slightly improved test accuracy (94.79%) but with increasing overfitting, as seen in rising validation loss.\n",
        "   - **Weakness**: Test loss was also higher, showing that the model's error on unseen data was increasing.\n",
        "\n",
        "3. **30 Epochs**:\n",
        "   - **Strength**: Best test accuracy (95.01%) but clear signs of overfitting.\n",
        "   - **Weakness**: Highest validation loss across epochs, and although the accuracy improved, the model made more mistakes when dealing with unseen data, as reflected in the higher loss.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**:\n",
        "\n",
        "- **Optimal Number of Epochs**: Based on this experiment, **30 epochs** gave the best test accuracy (**95.01%**), but at the cost of significantly higher validation loss, which indicates overfitting.\n",
        "  \n",
        "- **Best Trade-off**: **10 epochs** might offer the best balance between avoiding overfitting and maintaining decent performance, given the reasonable validation loss and accuracy.\n",
        "\n",
        "- **Next Steps**:\n",
        "   - If I wanted to further optimise the model, I could try **regularisation techniques** (like dropout) or **early stopping** to prevent the model from overfitting with more epochs.\n",
        "   - Another approach would be to adjust the **learning rate** to see if slower learning could lead to better generalisation over more epochs.\n",
        "\n",
        "Would you like me to explore those ideas further or try something else?"
      ],
      "metadata": {
        "id": "3umqhsD3lSpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflect\n",
        "\n",
        "Write a brief paragraph highlighting your process and the rationale to showcase critical thinking and problem-solving.\n",
        "\n",
        "Throughout this process, I aimed to identify the optimal combination of **batch size** and **epochs** for my spam classification model using TensorFlow. I began by experimenting with different **batch sizes** (16, 32, 64), evaluating how larger batches affected both model accuracy and overfitting. Then, I experimented with different numbers of **epochs** (10, 20, 30), observing how prolonged training impacted the model's ability to generalise to unseen data. By analysing test accuracy and loss, I was able to balance performance and overfitting, critically identifying that while more epochs improved accuracy, they also increased overfitting, especially with larger batch sizes. This iterative approach allowed me to not only improve accuracy but also develop an understanding of the trade-offs involved in hyperparameter tuning. Ultimately, I concluded that **batch size of 64** with **30 epochs** gave the best accuracy, while a more conservative combination of **10 epochs** with the same batch size offered a better balance between performance and overfitting."
      ],
      "metadata": {
        "id": "qjSjrsKPXtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Hopkins, M., Reeber, E., Forman, G., Suermondt, J., 1999. Spambase. [online]. Available at: https://archive.ics.uci.edu/dataset/94. [Accessed 5 March 2024]."
      ],
      "metadata": {
        "id": "4PPpUhWdek8D"
      }
    }
  ]
}