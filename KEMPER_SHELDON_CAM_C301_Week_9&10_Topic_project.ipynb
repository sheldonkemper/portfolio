{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/KEMPER_SHELDON_CAM_C301_Week_9%2610_Topic_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic project 9.1 Using time series analysis for sales and demand forecasting\n",
        "\n",
        "**Welcome to your topic project: Using time series analysis for sales and demand forecasting**\n",
        "\n",
        "In this project, you will bridge the gap between theory and practical application by utilising time series analysis techniques to forecast sales and demand. Forecasting involves analysing historical sales data to accurately predict future sales trends and demand patterns.\n",
        "\n",
        "\n",
        "Using time series analysis for sales and demand forecasting is crucial for data analysis in business because it can enhance decision-making by providing reliable sales and demand forecasts. This helps organisations reduce costs, maximise profits, and optimise resource allocation.\n",
        "\n",
        "\n",
        "During this project, you will analyse the business scenario and explore the data set. You will then clean historical sales data, perform time series analysis using ARIMA, deep learning, or hybrid methods, and validate the models to ensure their accuracy in predicting future sales and demand patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Please set aside approximately **36 hours** to complete the topic project.\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Business context**\n",
        "\n",
        "\n",
        "\n",
        "The Nielsen BookScan service is the world’s largest continuous book sales tracking service in the world, operating in the UK, Ireland, Australia, New Zealand, India, South Africa, Italy, Spain, Mexico, Brazil, Poland, and Colombia. Nielsen BookScan collects transactional data at the point of sale, directly from tills and dispatch systems of all major book retailers. This ensures detailed and highly accurate sales information on which books are selling and at what price, giving clients the most up-to-date and relevant data. The Nielsen BookScan Total Consumer Market (TCM) data covers approximately 90% of all retail print book purchases in the UK. The remaining sites are specialised, such as gift shops, specialist booksellers, and tourist information centres.\n",
        "\n",
        "\n",
        "Nielsen BookScan can be used to:\n",
        "Monitor titles and authors against the competition and overall market.\n",
        "Analyse pricing and discounting by format or category.\n",
        "Gauge the success of marketing campaigns and promotions.\n",
        "See which categories are growing and declining.\n",
        "Learn what works in your market and how that might differ from other countries.\n",
        "\n",
        "Nielsen BookScan sales data can be analysed by various criteria, including category, publisher, and format,\n",
        "allowing users to see which genres are selling in which format. Users can track market trends to see which titles are driving the results, and patterns can easily be interpreted. In addition, the actual selling price is included. This inclusion makes it easier to identify trends for the level of discounting (e.g. by title, author, genre, format, region, and publisher) when analysing book sales.\n",
        "\n",
        "**Project context**\n",
        "\n",
        "Nielsen is seeking to invest in developing a new service aimed at small to medium-sized independent publishers. This service is aimed at supporting publishers in using historical sales data to make data-driven decisions about their future investment in new publications. Their publisher customers are interested in being able to make more accurate predictions of the overall sales profile post-publication for better stock control and initial investment, but they are also interested in understanding the useful economic life span that a title may have.\n",
        "\n",
        "Nielsen is targeting small to medium-sized independent publishers as their research has shown that there is a strong demand for this insight, but businesses cannot invest in this infrastructure and would pay a premium to have access to quality-assured data and analysis in this area. Producing a new publication requires a significant upfront investment, and they would like to be able to more accurately identify books with strong long-term potential. More specifically, they are looking for titles with sales patterns that exhibit well-established seasonal patterns and positive trends that show potential great returns and to learn more about these types of publications. Nielsen will then apply this understanding to their commission and print volume strategy to be more successful in acquiring titles that have longevity. Additionally, this will enable them to deliver better returns by ensuring the correct stock levels in relation to demand and avoiding over- or understocking, which can be costly.\n",
        "\n",
        "You will notice that some titles experience fluctuations in sales due to various factors, such as increased media attention or cultural relevance (e.g. the recent resurgence of interest in George Orwell’s 1984). However, certain books endure over time and are often studied in academic settings for their deeper significance.\n",
        "\n",
        "For this project, Nielsen has provided you with two data sets. The objective is to identify sales patterns that demonstrate seasonal trends or any other traits, providing insights to inform reordering, restocking, and reprinting decisions for various books (by their International Standard Book Number, or ISBN).\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "By the end of this project, you will have conducted a comprehensive analysis on select books from Nielsen's data, identifying key sales patterns that exhibit clear seasonal trends or other distinctive characteristics. These insights will serve as a data-driven foundation for optimising procurement, re-ordering, and stocking decisions, ensuring efficient inventory management.\n",
        "\n",
        "\n",
        "In this Notebook, you will:\n",
        "- Import both datasets, resample weekly sales data to ensure missing weeks are filled with zeros, and convert ISBNs to strings and dates to datetime objects.\n",
        "- Filter out ISBNs with sales data beyond 2024-07-01, display these ISBNs, and plot their sales patterns for further analysis.\n",
        "- Investigate the general sales patterns across different time periods (1-12 years vs. 12-24 years) and analyse possible reasons for any noticeable changes.\n",
        "- Focus on two specific books, The Alchemist and The Very Hungry Caterpillar, for deeper analysis. Filter their data from 2012 onwards for further forecasting.\n",
        "- Perform time series decomposition, ACF/PACF analysis, and stationarity checks for both books. Use Auto ARIMA to identify the best model and forecast the final 32 weeks of data.\n",
        "- Prepare the data for machine learning models, create an XGBoost pipeline, tune parameters using grid search, and forecast the final 32 weeks for both books.\n",
        "- Build and tune an LSTM model using KerasTuner, forecast the final 32 weeks of data, and evaluate the performance using MAE and MAPE.\n",
        "- Apply both sequential and parallel hybrid models combining SARIMA and LSTM, tune the models, and evaluate their performance using various metrics.\n",
        "- Aggregate weekly data to monthly data, train XGBoost and SARIMA models to forecast eight months, and compare results with weekly predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You will also write a report summarising the results of your findings and recommendations.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "## **Assessment criteria**\n",
        "By completing this project, you will be able to provide evidence that you can:\n",
        "\n",
        "- Pre-process a real industry data set to make it suitable for time series analysis.\n",
        "- Apply various statistical tests to understand underlying characteristics of the data such as stationarity, seasonality and autocorrelation patterns.\n",
        "- Perform time series decomposition to separate the data into trend, seasonal and residual components.\n",
        "- Implement time series forecasting models.\n",
        "- Fine-tune the model to improve model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "## **Project guidance**\n",
        "\n",
        "**Import packages and data:**\n",
        "1. Load the appropriate libraries and models.\n",
        "2. Import the data set with the provided URL:\n",
        "  - Data set's Drive link: https://drive.google.com/drive/folders/1o8fZaaECKUnLYJAK4pqR4e3WyOFYhc_o\n",
        "  - Each file has four tabs, so ensure that you read the data from all four tabs.\n",
        "\n",
        "**Conducting initial data investigation:**\n",
        "\n",
        "1. Note that the data provided is weekly data. If no sales happened in a particular week, there will be no data representation for that week. This means that the data is not at fixed intervals.\n",
        "As a result, resample the data and fill in missing values with 0, such that even weeks with 0 sales is represented.\n",
        "2. Convert the ISBNs to a string value.\n",
        "3. Convert date to datetime object. (Recall that setting the date as the index has several advantages for time series handling.)\n",
        "4. Filter out the ISBNs (from all four tabs) wherein sales data exists beyond 2024-07-01. Show all the ISBNs that satisfy this criterion. Capture this in your report.\n",
        "5. Plot the data of all the ISBNs from the previous step by placing them in a loop.\n",
        "6. Investigate these plots to understand the general sales patterns, and comment on the general patterns visible in the data. Do the patterns drastically change for the period 1–12 years vs the period 12–24 years? Explain why or why not with possible reasons.\n",
        "7. Select two books from the list (*The Alchemist* and *The Very Hungry Caterpillar*) for further analysis. Focus on the period >2012-01-01. Filter the sales data for both these books to retain the date range >2012-01-01, until the final datapoint.\n",
        "\n",
        "**Classical techniques:**\n",
        "1. Perform decomposition on the data for both books. Determine what type of decomposition is suitable for each book, and comment on the components' characteristics.\n",
        "2. Perform ACF and PACF on both books. Comment on the results and what they indicate.\n",
        "3. Check for stationarity of the data for both books. Comment on the results and what they indicate.\n",
        "4. Perform Auto ARIMA on both books. **The forecast horizon is the final 32 weeks of data.** All prior data (from 2012-01-01 onwards) can be used as the training data. Set reasonable bounds for Auto ARIMA's various parameters so that Auto ARIMA can identify the most suitable model.\n",
        "5. Comment on the best model provided by Auto ARIMA for both books.\n",
        "6. Find the residuals of the 'best' model for both books. Comment on the residuals.\n",
        "7. Use the best model to predict the final 32 weeks of data. Plot the prediction along with the confidence intervals for both books.\n",
        "8. Comment on how the prediction compares with the actual values.\n",
        "\n",
        "**Machine learning and deep learning techniques**\n",
        "1. Prepare the data to feed into the machine learning models. The forecast horizon is **32 weeks**. The training data consists of all prior data, up to 2012-01-01.\n",
        "2. Create the required pipeline for the XGBoost model.\n",
        "3. Perform cross-validation.\n",
        "4. Perform parameter tuning (including window_length) using grid search.\n",
        "5. Identify the best models.\n",
        "6. Use the best models to forecast the **final 32 weeks** of sales data for both books.\n",
        "7. Plot the original data along with the predictions.\n",
        "8. Display the MAE and MAPE.\n",
        "9. Create an LSTM model for both books.\n",
        "10. Apply KerasTuner, and perform hyperparameter tuning using the training data, for both books.\n",
        "11. Use the best model to predict the **final 32 weeks** of data for both books.\n",
        "12. Plot the original data along with the predictions.\n",
        "13. Display the MAE and MAPE.\n",
        "\n",
        "**Hybrid model**\n",
        "1. Apply a hybrid model of SARIMA and LSTM in sequential combination wherein the residuals from SARIMA will be forecasted using LSTM. The final prediction will be the sum of the predictions from SARIMA and LSTM. The LSTM will be trained on the residuals obtained during the training of the SARIMA model. **The forecast horizon will be the final 32 weeks**. Use KerasTuner to get the best model. Plot the results. Display the MAE and MAPE, and comment on the results.\n",
        "2. Apply a hybrid model of SARIMA and LSTM in parallel combination wherein the predictions from SARIMA and the predictions from LSTM will be combined in the form of a weighted average. The final prediction will be the weighted sum of the predictions from SARIMA and LSTM. Both the SARIMA and the LSTM will be trained separately on the complete training set. **The forecast horizon will be the final 32 weeks**. Use KerasTuner to get the best model. Plot the results. Display the MAE and MAPE, and comment on the results.\n",
        "3. Modify the weightage in the parallel combination model to get different results. Find the weightage giving the best results, and comment on those results.\n",
        "\n",
        "**Monthly prediction**\n",
        "1. Instead of a weekly prediction, perform a monthly prediction. Aggregate the weekly sales data to monthly sales data for both books.\n",
        "2. Train the XGBoost model on this data. Forecast using the trained model. The forecast horizon is 8 months. Plot the results, and calculate the MAE and MAPE.\n",
        "3. Train the SARIMA model (using Auto ARIMA) on this data. Forecast using the trained model. The forecast horizon is 8 months. Plot the results, and calculate the MAE and MAPE.\n",
        "4. Compare and contrast the monthly predictions of both books against the weekly predictions. Comment on which one is more accurate or if they are the same.\n",
        "\n",
        "**Report:**\n",
        "1. Document your approach and major inferences from the data analysis.\n",
        "2. When you have completed the project:\n",
        "  - Download your completed Notebook as an IPYNB (Jupyter Notebook) or PY (Python) file. Save the file as follows: **LastName_FirstName_CAM_C301_Week_9and10_Topic_project.ipynb**.\n",
        "  - Prepare a detailed report (between 800–1,000 words) that includes:\n",
        "    - an overview of your approach\n",
        "    - a description of your analysis\n",
        "    - an explanation of the insights you identified\n",
        "    - any final insights, based on the output obtained from the various models employed.\n",
        "  - Save the document as a PDF named according to the following convention: **LastName_FirstName_CAM_C301_Week_9and10_Topic_project.pdf**.\n",
        "  - Submit your Notebook and PDF document.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "> **Declaration**\n",
        ">\n",
        "> By submitting your project, you indicate that the work is your own and has been created with academic integrity. Refer to the Cambridge plagiarism regulations."
      ],
      "metadata": {
        "id": "9j5Bse6izKKh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OW-0FMMsMze9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}