{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/6_2_2_Activity_AdaBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.\n",
        "Remember to update the portfolio index link to your own work once completed!"
      ],
      "metadata": {
        "id": "cbtFdN5VQsms"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXXUTFD7EtyU"
      },
      "source": [
        "# 6.2.2 Activity\n",
        "\n",
        "## Objective:\n",
        "\n",
        "Create a simple AdaBoost implementation and test it on the data set used in the demonstration video.\n",
        "\n",
        "## Activity guidance:\n",
        "\n",
        "* Create your own simple implementation of AdaBoost based on the steps provided in the lecture notes and demonstration video.\n",
        "* Use the provided data set, which is the same as the one used in the  demonstration.\n",
        "* Create a DataFrame that stores your predicted values at each iteration.\n",
        "* Compare your predictions against your training set to verify for overfitting.\n",
        "* Compare the results against the demonstration to verify the correct implementation.\n",
        "* (Optional) Implement your algorithm against the Iris data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "swMeGSH_EtyZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh11hXiLEtya"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "KPuHBNjG3SYb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CkTh4_HbEtyb"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "X = dict()\n",
        "X[\"a\"] = [2, 3, 5, 6, 7, 9, 10, 12, 13, 14]\n",
        "X[\"b\"] = [5, 1, 8, 7, 2, 12, 6, 11, 3, 10]\n",
        "X[\"target\"] = [1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
        "\n",
        "df = pd.DataFrame(X, columns=X.keys())\n",
        "\n",
        "# Features and target\n",
        "x = df[[\"a\", \"b\"]]\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Initialize equal weights\n",
        "df[\"weights_1\"] = 1 / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e-JBsPnXEtyb",
        "outputId": "e39101d4-5ccb-4b77-d519-a90cae5a15df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "  Error: 0.14285714285714288\n",
            "  Alpha: 0.8958797342640274\n",
            "  Weights: 0    0.056186\n",
            "7    0.337117\n",
            "2    0.056186\n",
            "9    0.137628\n",
            "4    0.137628\n",
            "3    0.137628\n",
            "6    0.137628\n",
            "Name: target, dtype: float64\n",
            "  Predictions: [1 0 1 0 0 0 0]\n",
            "\n",
            "Iteration 2:\n",
            "  Error: 0.11237243574396417\n",
            "  Alpha: 1.0333667853567925\n",
            "  Weights: 0    0.160108\n",
            "7    0.121617\n",
            "2    0.160108\n",
            "9    0.139542\n",
            "4    0.139542\n",
            "3    0.139542\n",
            "6    0.139542\n",
            "Name: target, dtype: float64\n",
            "  Predictions: [0 1 0 0 0 0 0]\n",
            "\n",
            "Iteration 3:\n",
            "  Error: 0.12161691309846019\n",
            "  Alpha: 0.9886033837342585\n",
            "  Weights: 0    0.059329\n",
            "7    0.325489\n",
            "2    0.059329\n",
            "9    0.138963\n",
            "4    0.138963\n",
            "3    0.138963\n",
            "6    0.138963\n",
            "Name: target, dtype: float64\n",
            "  Predictions: [1 0 1 0 0 0 0]\n",
            "\n",
            "Iteration 4:\n",
            "  Error: 0.11865758115876851\n",
            "  Alpha: 1.0026021721224692\n",
            "  Weights: 0    0.161908\n",
            "7    0.119589\n",
            "2    0.161908\n",
            "9    0.139149\n",
            "4    0.139149\n",
            "3    0.139149\n",
            "6    0.139149\n",
            "Name: target, dtype: float64\n",
            "  Predictions: [0 1 0 0 0 0 0]\n",
            "\n",
            "Iteration 5:\n",
            "  Error: 0.1195886217187714\n",
            "  Alpha: 0.9981657859314776\n",
            "  Weights: 0    0.059647\n",
            "7    0.324344\n",
            "2    0.059647\n",
            "9    0.139090\n",
            "4    0.139090\n",
            "3    0.139090\n",
            "6    0.139090\n",
            "Name: target, dtype: float64\n",
            "  Predictions: [1 0 1 0 0 0 0]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# AdaBoost function\n",
        "def AdaBoost(X, y, n_estimators=5):\n",
        "    n_samples = X.shape[0]\n",
        "    weights = np.ones(n_samples) / n_samples  # Initialize equal weights\n",
        "\n",
        "    models = []\n",
        "    alphas = []\n",
        "    predictions_at_each_iteration = pd.DataFrame(index=range(n_samples))\n",
        "\n",
        "    for i in range(n_estimators):\n",
        "        # Train a weak classifier (Decision Stump)\n",
        "        stump = DecisionTreeClassifier(max_depth=1)  # Decision stump (a tree with max depth of 1)\n",
        "        stump.fit(X, y, sample_weight=weights)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = stump.predict(X)\n",
        "        predictions_at_each_iteration[f\"Iteration_{i+1}\"] = predictions\n",
        "\n",
        "        # Calculate weighted error rate\n",
        "        error = np.sum(weights * (predictions != y)) / np.sum(weights)\n",
        "\n",
        "        # Calculate alpha (classifier weight)\n",
        "        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))  # Adding a small number to avoid division by zero\n",
        "\n",
        "        # Update weights\n",
        "        weights *= np.exp(-alpha * y * (2 * (predictions == y) - 1))  # Correct predictions get reduced weight\n",
        "        weights /= np.sum(weights)  # Normalize weights\n",
        "\n",
        "        # Store the stump and alpha\n",
        "        models.append(stump)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "        print(f\"Iteration {i+1}:\")\n",
        "        print(f\"  Error: {error}\")\n",
        "        print(f\"  Alpha: {alpha}\")\n",
        "        print(f\"  Weights: {weights}\")\n",
        "        print(f\"  Predictions: {predictions}\")\n",
        "        print()\n",
        "\n",
        "    return models, alphas, predictions_at_each_iteration\n",
        "\n",
        "# Run AdaBoost on the training data\n",
        "models, alphas, train_predictions = AdaBoost(x_train, y_train, n_estimators=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Zu65v-Etyc"
      },
      "source": [
        "# DataFrame with predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eFvLunYwEtyc",
        "outputId": "fdbfed0d-24d3-404e-8fe6-5248e34d5ac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data Evaluation:\n",
            "Accuracy: 0.8571428571428571\n",
            "Confusion Matrix:\n",
            "[[4 0]\n",
            " [1 2]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      1.00      0.89         4\n",
            "           1       1.00      0.67      0.80         3\n",
            "\n",
            "    accuracy                           0.86         7\n",
            "   macro avg       0.90      0.83      0.84         7\n",
            "weighted avg       0.89      0.86      0.85         7\n",
            "\n",
            "\n",
            "Test Data Evaluation:\n",
            "Accuracy: 0.6666666666666666\n",
            "Confusion Matrix:\n",
            "[[1 0]\n",
            " [1 1]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.75      0.75      0.67         3\n",
            "weighted avg       0.83      0.67      0.67         3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Final combined model prediction\n",
        "def predict_boosted(models, alphas, X):\n",
        "    final_predictions = np.zeros(X.shape[0])\n",
        "    for model, alpha in zip(models, alphas):\n",
        "        predictions = model.predict(X)\n",
        "        final_predictions += alpha * (2 * (predictions == 1) - 1)\n",
        "    return (final_predictions > 0).astype(int)\n",
        "\n",
        "# Predict on training and test data\n",
        "y_train_pred = predict_boosted(models, alphas, x_train)\n",
        "y_test_pred = predict_boosted(models, alphas, x_test)\n",
        "\n",
        "# Evaluate on training data\n",
        "print(\"\\nTraining Data Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_train, y_train_pred)}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "print(\"\\nTest Data Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_test_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH-OCa6kEtyc"
      },
      "source": [
        "# Check for overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w62IH3FuEtyd",
        "outputId": "d95904d9-1990-4411-aca9-dea17e3a9773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 - Training Accuracy: 0.8571428571428571\n",
            "Iteration 2 - Training Accuracy: 0.7142857142857143\n",
            "Iteration 3 - Training Accuracy: 0.8571428571428571\n",
            "Iteration 4 - Training Accuracy: 0.7142857142857143\n",
            "Iteration 5 - Training Accuracy: 0.8571428571428571\n",
            "Final Test Set Accuracy: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Accuracy on training set at each iteration\n",
        "for i in range(1, 6):\n",
        "    print(f\"Iteration {i} - Training Accuracy: {accuracy_score(y_train, train_predictions[f'Iteration_{i}'])}\")\n",
        "\n",
        "# Accuracy on test set using final boosted model\n",
        "print(f\"Final Test Set Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this AdaBoost experiment, I observed how the model's performance evolved over five iterations. Initially, the training accuracy was relatively high, around 86%, but it fluctuated across iterations, dropping to about 71% in some cases before recovering again. This variation is typical in AdaBoost, as the algorithm continually shifts its focus to the more challenging examples that were previously misclassified.\n",
        "\n",
        "Interestingly, while the training accuracy bounced back to 86% by the final iteration, the model's ability to generalise was somewhat limited, with the test set accuracy sitting at 66.67%. This discrepancy between training and test accuracy suggests signs of overfitting, where the model becomes overly specialised to the training data but struggles to perform as well on unseen data.\n",
        "\n",
        "The alternating training accuracy and lower test accuracy highlight the trade-off AdaBoost faces—its iterative focus on difficult examples can lead to stronger performance on the training set but may come at the cost of generalisation.\n",
        "\n",
        "In future, I'd look to address this by perhaps increasing the dataset size, experimenting with regularisation techniques, or fine-tuning parameters like the number of iterations or base learner depth."
      ],
      "metadata": {
        "id": "QIs1be195WAe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVW33I8dEtyd"
      },
      "source": [
        "# (Optional) Run your algorithm against the Iris dataset\n",
        "\n",
        "Make sure to do the appropriate preprocessing of the data before  running it through your algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_PTB7xtEtyd"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "x, y = load_iris(return_X_y=True, as_frame=True)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pHp3wodEtye"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rXXUTFD7EtyU",
        "7_Zu65v-Etyc"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}