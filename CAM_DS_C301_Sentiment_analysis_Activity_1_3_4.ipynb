{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/CAM_DS_C301_Sentiment_analysis_Activity_1_3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 1.3.4 Sentiment analysis of movie reviews\n",
        "\n",
        "## Scenario\n",
        "As part of a market research exercise for a film studio planning a new science-fiction film, you have been tasked with a data science project to research customer feedback on films in a related genre. One question you will be asked to investigate is whether there’s a relationship between the proportion of feedback that is positive and production budgets. Before you compare sentiment scores between films, however, you need to construct a viable preprocessing pipeline and train a model.\n",
        "\n",
        "\n",
        "## Objective\n",
        "In this portfolio activity, you will apply what you have learned about data preprocessing for NLP as well as some technique for calculating similarity.\n",
        "\n",
        "\n",
        "## Assessment criteria\n",
        "\n",
        "By completing this activity, you will be able to provide evidence that you can:\n",
        "*   construct an NLP classification model capable of predicting binary sentiment class (positive or negative) with high accuracy\n",
        "*   perform supporting calculations including cosine similarity to enable broad discussion of the techniques, and evaluate model performance\n",
        "*   report your findings in language that a non-specialist would understand.\n",
        "\n",
        "\n",
        "## Activity guidance\n",
        "\n",
        "1. Install the necessary packages that will be useful in this  activity.\n",
        "2. Load the dataset sst2 from Hugging Face (https://huggingface.co/datasets/sst2).\n",
        "3. Create dataframes of the train and validation split.\n",
        "4. Calculate the cosine similarity of the 5th and 100th sentence within the train split.\n",
        "5. Calculate the cosine similarity of the 5th and 15,000th sentence within the train split.\n",
        "6. Calculate the cosine similarity of the 5th and 50,000th sentence within the train split.\n",
        "7. Comment on the cosine similarity scores.\n",
        "8. Create a preprocessing function to perform several processing steps as described below to the train and validation texts:\n",
        " - Remove any punctuation and html tags.\n",
        " - Tokenize the text into tokens.\n",
        " - Remove stop words from your text.\n",
        " - Perform lemmatisation and stemming on your text (one at a time).\n",
        "\n",
        "9. Obtain Bag-of-Word and TF-IDF representations for both the train and validation splits.\n",
        "10. Train a logistic regression model using scikit-learn first with the Bag-of-Words and then TF-IDF, and report the performance of the sentiment classifier.\n"
      ],
      "metadata": {
        "id": "3evNuEt42PM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Start your activity here. Select the pen from the toolbar to add your entry."
      ],
      "metadata": {
        "id": "2BMsMzEZ2Wcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GIobewkxlI6k"
      },
      "outputs": [],
      "source": [
        "#In this activity, you will be required to download a data set from Hugging Face and perfom the text classification on the the data set.\n",
        "#You will be required to study the impact of different different parameter choices on the classification perfomance of sentiment classifier.\n",
        "\n",
        "\n",
        "#1. Install the necessary packages that will be useful in this activity.\n",
        "#2. Load the data set sst2 from Hugging Face (https://huggingface.co/datasets/sst2).\n",
        "#3. Create dataframes of the train and validation split.\n",
        "#4. Calculate the cosine similarity of the 5th and 100th sentence within the train split.\n",
        "#5. Calculate the cosine similarity of the 5th and 15,000th sentence within the train split.\n",
        "#6. Calculate the cosine similarity of the 5th and 50,000th sentence within the train split.\n",
        "#7. Comment on the cosine similarity scores.\n",
        "#8. Create a preprocessing function to perform several processing steps as described below to the train and validation texts:\n",
        "     #1. Remove any punctuation and HTML tags.\n",
        "     #2. Tokenise the text into tokens.\n",
        "     #3. Remove stop words from your text.\n",
        "     #4. Perform lemmatisation and stemming on your text (one at a time).\n",
        "\n",
        "#9.  Obtain Bag-of-Word and TF-IDF  Representation for both the train and validation splits.\n",
        "#10. Train a logistic regression model using scikit-learn first with the Bag-of-Words and then TF-IDF, and report the performance of the sentiment classifier.\n",
        "#11. What is the impact of not removing stop words on the performance of the sentiment classifier?\n",
        "#12. Which one is more important to the perfomrance of the sentiment classifier (lemmatisation or stemming)?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ojhkIwBq2MWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Install the necessary packages that will be useful in this activity.\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install beautifulsoup4\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEdewk-vgxVg",
        "outputId": "0975082c-5994-4ff5-a8bb-d248b0bf635e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy"
      ],
      "metadata": {
        "id": "lPPHVdUSeGlr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load the Dataset from Hugging Face\n"
      ],
      "metadata": {
        "id": "qIg5knIcfK37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Load the data set sst2 from Hugging Face (https://huggingface.co/datasets/sst2).\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"sst2\")"
      ],
      "metadata": {
        "id": "0L2xEjsIhHPa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQrRsAy5hiGX",
        "outputId": "ded0f813-ba98-430f-e8c2-a3547bcc7c86"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 67349\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['idx', 'sentence', 'label'],\n",
              "        num_rows: 1821\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset I worked with is structured into three distinct splits: **train**, **validation**, and **test**. Here's the breakdown:\n",
        "\n",
        "- The **train** dataset contains **67,349** rows. Each row features an index (`idx`), a sentence (`sentence`), and a label (`label`).\n",
        "- The **validation** dataset has **872** rows, similarly structured with the same features.\n",
        "- The **test** dataset includes **1,821** rows, also containing the `idx`, `sentence`, and `label` fields.\n",
        "\n",
        "This distribution indicates that the bulk of the data is in the training set, providing a comprehensive basis for model training, while the validation and test sets are used for evaluation and tuning."
      ],
      "metadata": {
        "id": "7MusPZhqudJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create DataFrames for the Train and Validation Split"
      ],
      "metadata": {
        "id": "rSdI297ofG4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Create dataframes of the train and validation split.\n",
        "\n",
        "# Convert the train and validation splits to dataframes\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "validation_df = pd.DataFrame(dataset['validation'])\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "2_lZ4v73hdOa",
        "outputId": "8617f8ea-94b0-4200-855c-03808ec1d4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   idx                                           sentence  label\n",
              "0    0       hide new secretions from the parental units       0\n",
              "1    1               contains no wit , only labored gags       0\n",
              "2    2  that loves its characters and communicates som...      1\n",
              "3    3  remains utterly satisfied to remain the same t...      0\n",
              "4    4  on the worst revenge-of-the-nerds clichés the ...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df4bce46-0764-4cfe-8cac-3f276441cfd4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df4bce46-0764-4cfe-8cac-3f276441cfd4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df4bce46-0764-4cfe-8cac-3f276441cfd4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df4bce46-0764-4cfe-8cac-3f276441cfd4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc296f93-1aaf-4d71-b1d8-92691b5be9b7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc296f93-1aaf-4d71-b1d8-92691b5be9b7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc296f93-1aaf-4d71-b1d8-92691b5be9b7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 67349,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19442,\n        \"min\": 0,\n        \"max\": 67348,\n        \"num_unique_values\": 67349,\n        \"samples\": [\n          66730,\n          29890,\n          45801\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 66978,\n        \"samples\": [\n          \"mr. day-lewis roars with leonine power \",\n          \"a fairly slow paced , almost humdrum approach to character development \",\n          \"it 's not very interesting . \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Calculate the Cosine Similarity of the 5th and 100th Sentence"
      ],
      "metadata": {
        "id": "_8dBzav9fBv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "# Select the 5th and 100th sentence\n",
        "sentence_5 =  train_df['sentence'][4]\n",
        "sentence_100 = train_df['sentence'][99]\n",
        "# Calculate the cosine similarity\n",
        "doc1 = nlp(sentence_5)\n",
        "doc2 = nlp(sentence_100)\n",
        "cosine_similarity_5_100 = doc1.similarity(doc2)\n",
        "\n",
        "print(f\"Cosine similarity between the 5th and 100th sentence: {cosine_similarity_5_100}\")\n"
      ],
      "metadata": {
        "id": "LxHuxHNTeVoU",
        "outputId": "f0000eb7-8218-417d-8283-f52cd6a94a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between the 5th and 100th sentence: 0.7073607748427319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Calculate the Cosine Similarity of the 5th and 15,000th Sentence\n"
      ],
      "metadata": {
        "id": "XEWwktdnfSGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_15000 = train_df['sentence'][min(14999,len(train_df)-1)]\n",
        "doc2 = nlp(sentence_15000)\n",
        "cosine_similarity_5_15000 = doc1.similarity(doc2)\n",
        "\n",
        "print(f\"Cosine similarity between the 5th and 15,000th sentence: {cosine_similarity_5_15000}\")"
      ],
      "metadata": {
        "id": "nJDYWvbDe-o9",
        "outputId": "2e3db889-dfc2-45f2-bc96-8a6cca14226e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between the 5th and 15,000th sentence: 0.2167132018837024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Calculate the Cosine Similarity of the 5th and 50,000th Sentence"
      ],
      "metadata": {
        "id": "eQ9-1R5Rf7q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_50000 = train_df['sentence'][min(14999,len(train_df)-1)]\n",
        "doc2 = nlp(sentence_50000)\n",
        "cosine_similarity_5_50000 = doc1.similarity(doc2)\n",
        "\n",
        "print(f\"Cosine similarity between the 5th and 50,000th sentence: {cosine_similarity_5_50000}\")"
      ],
      "metadata": {
        "id": "z0KxSB50f8go",
        "outputId": "bf93e069-098f-4a6f-df85-f7d1335ad4b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between the 5th and 50,000th sentence: 0.2167132018837024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Comment on the Cosine Similarity Scores\n"
      ],
      "metadata": {
        "id": "pKuZvQ-ygS8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The comments will be based on the cosine similarity scores calculated above.\n",
        "# Higher scores indicate higher similarity between the sentences.\n",
        "\n",
        "print(\"Cosine Similarity Observations:\")\n",
        "print(f\"Cosine similarity between the 5th and 100th sentence: {cosine_similarity_5_100}\")\n",
        "print(f\"Cosine similarity between the 5th and 15,000th sentence: {cosine_similarity_5_15000}\")\n",
        "print(f\"Cosine similarity between the 5th and 50,000th sentence: {cosine_similarity_5_50000}\")\n",
        "\n",
        "# Comments\n"
      ],
      "metadata": {
        "id": "a09RNEFBgVCa",
        "outputId": "f3c0f107-61f5-4887-b9b2-373dc7d44f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Observations:\n",
            "Cosine similarity between the 5th and 100th sentence: 0.7073607748427319\n",
            "Cosine similarity between the 5th and 15,000th sentence: 0.2167132018837024\n",
            "Cosine similarity between the 5th and 50,000th sentence: 0.2167132018837024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity Observations:\n",
        "\n",
        "1. **Cosine similarity between the 5th and 100th sentence: 0.707**  \n",
        "   This value indicates a moderate to high similarity, suggesting that these two sentences likely share a significant number of words or have a similar context. It implies that the themes or the language used in these sentences might be closely related.\n",
        "\n",
        "2. **Cosine similarity between the 5th and 15,000th sentence: 0.217**  \n",
        "   The lower value here indicates that these sentences are not very similar. The sentences may cover different topics, themes, or use distinct vocabulary, highlighting diversity within the dataset.\n",
        "\n",
        "3. **Cosine similarity between the 5th and 50,000th sentence: 0.217**  \n",
        "   This similarity score is identical to the previous one, showing that the 5th and 50,000th sentences are also quite different. It suggests that as the dataset progresses, the variety in sentence structure or topic increases, leading to lower similarities when compared with the early entries."
      ],
      "metadata": {
        "id": "SaKc7JErvV8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Create a Preprocessing Function\n"
      ],
      "metadata": {
        "id": "be5hMaP9gx3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text, method=\"lemmatization\"):\n",
        "     # Ensure the text is a string\n",
        "    text = str(text)\n",
        "    # Remove HTML tags\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize or stem\n",
        "    if method == \"lemmatization\":\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    else:  # Stemming\n",
        "        processed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    return ' '.join(processed_tokens)\n"
      ],
      "metadata": {
        "id": "brA956DWhBUu",
        "outputId": "b66faa1c-44a6-45a7-f4e4-ccd44a82c379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Obtain Bag-of-Words and TF-IDF Representations\n"
      ],
      "metadata": {
        "id": "fDw3ZC43icGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing function to train and validation text\n",
        "train_df['clean_text_lemmatization'] = train_df['sentence'].apply(lambda x: preprocess_text(x))\n",
        "validation_df['clean_text_lemmatization'] = validation_df['sentence'].apply(lambda x: preprocess_text(x))\n"
      ],
      "metadata": {
        "id": "wn_7mnEQidO9",
        "outputId": "1513f3e3-c884-4c04-8b64-21ee39ce8752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-e175b3168d15>:21: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag-of-Words Vectorization\n",
        "bow_vectorizer = CountVectorizer(max_features=3000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_df['clean_text_lemmatization'])\n",
        "X_val_bow = bow_vectorizer.transform(validation_df['clean_text_lemmatization'])\n"
      ],
      "metadata": {
        "id": "nw9lzlYGkMqQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['clean_text_lemmatization'])\n",
        "X_val_tfidf = tfidf_vectorizer.transform(validation_df['clean_text_lemmatization'])"
      ],
      "metadata": {
        "id": "0ipuGpmakiMI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Train a Logistic Regression Model Using Bag-of-Words and TF-IDF"
      ],
      "metadata": {
        "id": "eVX8vBVZk-j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "6fydLdyHn2Wn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with Bag-On-Words\n",
        "lr_bow = LogisticRegression(max_iter=1000)\n",
        "lr_bow.fit(X_train_bow, train_df['label'])\n",
        "y_pred_bow = lr_bow.predict(X_val_bow)\n",
        "\n",
        "print(\"Bag-of-Words Model Performance:\")\n",
        "print(classification_report(validation_df['label'], y_pred_bow))"
      ],
      "metadata": {
        "id": "lh6FQ44xlO7M",
        "outputId": "9577a0ef-8049-4537-8251-c1538a0975f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.71      0.76       428\n",
            "           1       0.75      0.84      0.79       444\n",
            "\n",
            "    accuracy                           0.78       872\n",
            "   macro avg       0.78      0.78      0.78       872\n",
            "weighted avg       0.78      0.78      0.78       872\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's precision is 0.81 for negative sentiments and 0.75 for positive sentiments. This means that when it predicts a sentiment, it’s mostly accurate in identifying both negative and positive cases.\n",
        "In terms of recall, it’s 0.71 for negative sentiments and 0.84 for positive ones, so it's quite effective, especially in picking up positive instances.\n",
        "The F1-scores—which balance precision and recall—are 0.76 for negative and 0.79 for positive. These scores show that the model maintains a balanced performance overall.\n",
        "The overall accuracy is 78%, which I think is pretty solid for a first attempt.\n"
      ],
      "metadata": {
        "id": "TCczYUpcxhn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Evaluate the Impact of Not Removing Stop Words\n",
        "\n"
      ],
      "metadata": {
        "id": "vm4gmbPklnIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate without removing stop words\n",
        "def preprocess_no_stopwords(text, method=\"lemmatization\"):\n",
        "    text = str(text)\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    if method == \"lemmatization\":\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    else:\n",
        "        processed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(processed_tokens)\n",
        "\n",
        "# Apply the no-stopwords preprocessing\n",
        "train_df['clean_text_no_stop'] = train_df['sentence'].apply(lambda x: preprocess_no_stopwords(x))\n",
        "validation_df['clean_text_no_stop'] = validation_df['sentence'].apply(lambda x: preprocess_no_stopwords(x))\n",
        "\n",
        "# Vectorize\n",
        "X_train_no_stop = tfidf_vectorizer.fit_transform(train_df['clean_text_no_stop'])\n",
        "X_val_no_stop = tfidf_vectorizer.transform(validation_df['clean_text_no_stop'])\n",
        "\n",
        "# Train and evaluate\n",
        "clf_no_stop = LogisticRegression(max_iter=1000)\n",
        "clf_no_stop.fit(X_train_no_stop, train_df['label'])\n",
        "y_pred_no_stop = clf_no_stop.predict(X_val_no_stop)\n",
        "\n",
        "print(\"Model Performance Without Removing Stop Words:\")\n",
        "print(classification_report(validation_df['label'], y_pred_no_stop))\n"
      ],
      "metadata": {
        "id": "1jYh22bplrYL",
        "outputId": "53859b30-5e87-444e-ea16-1ee5f8ad7992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-923f6351d5b1>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance Without Removing Stop Words:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.76      0.78       428\n",
            "           1       0.78      0.82      0.80       444\n",
            "\n",
            "    accuracy                           0.79       872\n",
            "   macro avg       0.79      0.79      0.79       872\n",
            "weighted avg       0.79      0.79      0.79       872\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The **precision** for negative sentiments remained at **0.81**, while for positive sentiments, it increased slightly to **0.78**. This indicates that the model still accurately identifies sentiments, even with common words included.\n",
        "- The **recall** for negative sentiments improved to **0.76** and for positive sentiments was **0.82**, showing that the model is slightly more balanced in detecting both types of sentiments without stop word removal.\n",
        "- The **F1-scores** were **0.78** for negative and **0.80** for positive, which are similar but indicate a slight improvement, particularly for the positive class.\n",
        "- The overall **accuracy** is **79%**, a small increase from the previous Bag-of-Words model where stop words were removed.\n",
        "\n",
        "In summary, not removing stop words slightly improved the model’s performance, particularly in detecting positive instances. This suggests that, in some cases, including these common words might help the model better understand the context of sentiments. It's interesting to see that such a small tweak can have a noticeable impact!"
      ],
      "metadata": {
        "id": "C83dhbbbyOMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12: Compare Lemmatization vs. Stemming\n"
      ],
      "metadata": {
        "id": "talqPG1EmICm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate with stemming\n",
        "train_df['clean_text_stemming'] = train_df['sentence'].apply(lambda x: preprocess_text(x, method=\"stemming\"))\n",
        "validation_df['clean_text_stemming'] = validation_df['sentence'].apply(lambda x: preprocess_text(x, method=\"stemming\"))\n",
        "\n",
        "X_train_stemming = tfidf_vectorizer.fit_transform(train_df['clean_text_stemming'])\n",
        "X_val_stemming = tfidf_vectorizer.transform(validation_df['clean_text_stemming'])\n",
        "\n",
        "clf_stemming = LogisticRegression(max_iter=1000)\n",
        "clf_stemming.fit(X_train_stemming, train_df['label'])\n",
        "y_pred_stemming = clf_stemming.predict(X_val_stemming)\n",
        "\n",
        "print(\"Stemming Model Performance:\")\n",
        "print(classification_report(validation_df['label'], y_pred_stemming))\n",
        "\n",
        "# Compare with lemmatization\n",
        "print(\"Lemmatization Model Performance:\")\n",
        "print(classification_report(validation_df['label'], y_pred_bow))\n"
      ],
      "metadata": {
        "id": "zq5eSJFumI4R",
        "outputId": "246c72ae-6b97-432c-fa08-75c0c8415948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-f1e23fd362bb>:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "<ipython-input-68-f1e23fd362bb>:19: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.70      0.76       428\n",
            "           1       0.75      0.86      0.80       444\n",
            "\n",
            "    accuracy                           0.78       872\n",
            "   macro avg       0.79      0.78      0.78       872\n",
            "weighted avg       0.79      0.78      0.78       872\n",
            "\n",
            "Lemmatization Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.71      0.76       428\n",
            "           1       0.75      0.84      0.79       444\n",
            "\n",
            "    accuracy                           0.78       872\n",
            "   macro avg       0.78      0.78      0.78       872\n",
            "weighted avg       0.78      0.78      0.78       872\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Stemming Model Performance\n",
        "- **Precision**: For negative sentiments, precision is **0.83**, and for positive sentiments, it's **0.75**. This shows that the model is quite accurate in identifying negative sentiments with stemming.\n",
        "- **Recall**: The recall for negative sentiments is **0.70**, while for positive sentiments, it's higher at **0.86**. The model is better at capturing positive sentiments but slightly less effective with negative ones.\n",
        "- **F1-Score**: The F1-score for negative sentiments is **0.76**, and for positive, it's **0.80**, reflecting the trade-off between precision and recall.\n",
        "- **Overall Accuracy**: The model achieved **78%** accuracy.\n",
        "\n",
        "### Lemmatization Model Performance\n",
        "- **Precision**: The precision for negative sentiments is **0.81**, and for positive sentiments, it's **0.75**, slightly lower than the stemming model for negative sentiments but similar for positive ones.\n",
        "- **Recall**: The recall for negative sentiments is **0.71**, while for positive, it's **0.84**. Similar to the stemming model, it has higher recall for positive instances.\n",
        "- **F1-Score**: The F1-score for negative is **0.76**, and for positive, it's **0.79**, showing a balanced approach between precision and recall.\n",
        "- **Overall Accuracy**: The accuracy remains consistent at **78%**.\n",
        "\n",
        "### Summary\n",
        "Both models achieved similar accuracy scores, but the stemming model shows higher precision for negative sentiments, while the lemmatization model maintains a balanced precision-recall ratio across both sentiment classes. Stemming seems to be slightly more effective in capturing the distinct features of negative sentiments, while lemmatization offers a balanced approach."
      ],
      "metadata": {
        "id": "VanownOoykhq"
      }
    }
  ]
}