{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/CAM_DS__C201_Activity_2_3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 2.3.4: Performing manual propagation\n",
        "\n",
        "## Scenario\n",
        "A tech startup developing a recommendation engine for online retail needs its neural network created and optimised for maximum accuracy and efficiency. As a data scientist, you’ve been tasked with creating the network, initialising weights and biases, defining input variables, selecting activation functions, computing gradients, and using gradient descent to fine-tune the model. The end result will be a model with improved recommendation accuracy that drives customer engagement in the online retail sector.\n",
        "\n",
        "## Objective\n",
        "Manually complete a forward and backward pass in a neural network.\n",
        "\n",
        "You’ll complete the activity in your Notebook, where you’ll:\n",
        "- create a simple neural network\n",
        "- initialise weights and biases\n",
        "- provide input variables\n",
        "- define activation functions and their derivatives\n",
        "- compute hidden-layer gradients\n",
        "- use gradient descent to optimise weights and biases.\n",
        "\n",
        "## Assessment criteria\n",
        "By completing this activity, you'll be able to provide evidence that you can create a simple neural network and complete a forward and backward pass.\n",
        "\n",
        "## Activity guidance\n",
        "1. Import the relevant libraries, including keras.models, keras.layers, and keras.utils.\n",
        "2. Create a neural network with an input dimension of 2, one hidden layer, and one output layer.\n",
        "  - For the hidden layer, initialise the weights as a 2x2 matrix, where the rows and columns of the matrix are np.array([0.2, 0.9], [0.6, 0.6]).\n",
        "  - For the hidden layer, initialise the bias vectors as np.array([0.8, 0.9]).\n",
        "  - For the output layer, initialise the weights as a 1x2 matrix, with weights np.array([[0.9], [0.4]]).\n",
        "  - For the output layer, initialise the bias neuron as np.array([0.9]).\n",
        "3. Define the input data as the variable X made of two elements: 0.3 and 0.4.\n",
        "4. Define the ReLu function.\n",
        "5. Print the input data, hidden layer output, and final output.\n",
        "6. Define the ReLu and Sigmoid functions and also their derivatives.\n",
        "7. Assume the target for binary classification is 1 and save it in a variable called target.\n",
        "8. Compute the loss derivative with respect to the final output.\n",
        "9. Complete a backward pass through the output layer.\n",
        "10. Compute the gradients for the output layer parameters.\n",
        "11. Complete a backward pass through the hidden layer.\n",
        "12. Compute the gradients for the hidden layer parameters.\n",
        "13. Set a simple learning rate of 0.02.\n",
        "14. Update the weights and biases using gradient descent.\n",
        "15. Print the updated weights and biases."
      ],
      "metadata": {
        "id": "ltwXKT0TCvf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qE2gU4Q1an1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import modules"
      ],
      "metadata": {
        "id": "zVdC2vpRaKXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "rrKpiGJ4aJd1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define four functions that are necessary for computing derivatives.\n"
      ],
      "metadata": {
        "id": "nLbMvK00YUrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ReLU and Sigmoid activation functions and their derivatives\n",
        "\n",
        "# ReLU (Rectified Linear Unit) Activation Function:\n",
        "# ReLU returns the input directly if it is positive; otherwise, it returns 0.\n",
        "# This is commonly used in hidden layers as it helps introduce non-linearity into the model.\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLU:\n",
        "# The derivative of ReLU is 1 if the input is positive, and 0 if the input is less than or equal to 0.\n",
        "# This derivative is used in backpropagation to compute gradients.\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Sigmoid Activation Function:\n",
        "# Sigmoid squashes any input to a value between 0 and 1, often used in the output layer for binary classification problems.\n",
        "# It can be interpreted as the probability of an output being 1 (True).\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid:\n",
        "# The derivative of Sigmoid is calculated as sigmoid(x) * (1 - sigmoid(x)).\n",
        "# This is used during backpropagation to compute how much the weights should be adjusted.\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n"
      ],
      "metadata": {
        "id": "QiBg9QbNYTBo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initialize weights and biases for the hidden and output layers\n"
      ],
      "metadata": {
        "id": "KJnBkx9TVwcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hidden layer weights (2x2 matrix)\n",
        "W_hidden = np.array([[0.2, 0.9], [0.6, 0.6]])\n",
        "# Hidden layer bias (2x1 vector)\n",
        "b_hidden = np.array([0.8, 0.9])\n",
        "\n",
        "# Output layer weights (1x2 matrix)\n",
        "W_output = np.array([[0.9], [0.4]])\n",
        "\n",
        "# Output layer bias (1x1 scalar)\n",
        "b_output = np.array([0.9])"
      ],
      "metadata": {
        "id": "S2XMokfBVfb7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define the input data (X)"
      ],
      "metadata": {
        "id": "-9sALQa8V4ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([0.3, 0.4])"
      ],
      "metadata": {
        "id": "K1B1y_hTV5iw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Perform the forward pass"
      ],
      "metadata": {
        "id": "LqY7HCn0WDXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute hidden layer input (W_hidden * X + b_hidden)\n",
        "hidden_input = np.dot(X, W_hidden) + b_hidden\n",
        "\n",
        "# Compute hidden layer output using ReLU activation\n",
        "hidden_output = relu(hidden_input)\n",
        "\n",
        "# Compute output layer input (W_output * hidden_output + b_output)\n",
        "output_input = np.dot(hidden_output, W_output) + b_output\n",
        "\n",
        "# Compute the final output using Sigmoid activation (for binary classification)\n",
        "final_output = sigmoid(output_input)"
      ],
      "metadata": {
        "id": "ES6DxfY5WB2e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Print the input data, hidden layer output, and final output"
      ],
      "metadata": {
        "id": "a_BPMebLX7pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input Data (X):\", X)\n",
        "print(\"Hidden Layer Output:\", hidden_output)\n",
        "print(\"Final Output:\", final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhYpkbWEX6E0",
        "outputId": "c82366fd-d726-48cf-a0eb-12a3bf4a0e59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data (X): [0.3 0.4]\n",
            "Hidden Layer Output: [1.1  1.41]\n",
            "Final Output: [0.92085347]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Define the target value (binary classification target)"
      ],
      "metadata": {
        "id": "fuqDYfeGYFW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = 1  # The target label"
      ],
      "metadata": {
        "id": "bPn9PQFjYD_5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Compute the loss derivative with respect to the final output"
      ],
      "metadata": {
        "id": "CFfrnmc9ZDAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_derivative = final_output - target"
      ],
      "metadata": {
        "id": "qUQECNwwZBoF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Perform the backward pass through the output layer"
      ],
      "metadata": {
        "id": "lViYvoLxZSU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradients for the output layer weights and bias\n",
        "d_output_input = loss_derivative * sigmoid_derivative(output_input)\n",
        "grad_W_output = np.dot(hidden_output.reshape(-1, 1), d_output_input.reshape(1, -1))\n",
        "grad_b_output = d_output_input"
      ],
      "metadata": {
        "id": "Ia43ey_gZQxi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Perform the backward pass through the hidden layer\n"
      ],
      "metadata": {
        "id": "jBXupkAcZZAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradients for the hidden layer weights and biases\n",
        "d_hidden_output = np.dot(d_output_input, W_output.T) * relu_derivative(hidden_input)\n",
        "grad_W_hidden = np.dot(X.reshape(-1, 1), d_hidden_output.reshape(1, -1))\n",
        "grad_b_hidden = d_hidden_output"
      ],
      "metadata": {
        "id": "8jRP9KKrZXnv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Set the learning rate and update weights and biases using gradient descent\n"
      ],
      "metadata": {
        "id": "PAaMyS1GZmhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.02\n",
        "\n",
        "# Update the weights and biases\n",
        "W_output -= learning_rate * grad_W_output\n",
        "b_output -= learning_rate * grad_b_output\n",
        "W_hidden -= learning_rate * grad_W_hidden\n",
        "b_hidden -= learning_rate * grad_b_hidden"
      ],
      "metadata": {
        "id": "WDsy8FLAZlHi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Print the updated weights and biases"
      ],
      "metadata": {
        "id": "zLoq_pECZynz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Updated Hidden Layer Weights:\", W_hidden)\n",
        "print(\"Updated Hidden Layer Biases:\", b_hidden)\n",
        "print(\"Updated Output Layer Weights:\", W_output)\n",
        "print(\"Updated Output Layer Biases:\", b_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ5pQGsbZq3C",
        "outputId": "be9ff7f3-2709-4f5c-bdc1-346cfade206a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Hidden Layer Weights: [[0.20003115 0.90001384]\n",
            " [0.60004153 0.60001846]]\n",
            "Updated Hidden Layer Biases: [0.80010383 0.90004615]\n",
            "Updated Output Layer Weights: [[0.9001269 ]\n",
            " [0.40016267]]\n",
            "Updated Output Layer Biases: [0.90011537]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflect\n",
        "\n",
        "In this activity, I manually performed forward and backward propagation in a simple neural network, ensuring a thorough understanding of each step. The process began by defining the input data and initializing the weights and biases for both the hidden and output layers. I applied the ReLU activation function to the hidden layer and the Sigmoid function to the output layer, ensuring the correct non-linear transformations. During forward propagation, I calculated the output, and in the backward pass, I used gradient descent to adjust the weights and biases based on the loss derivatives. My rationale was to break down each step to better grasp how neural networks learn and optimize by minimizing errors, demonstrating my critical thinking and problem-solving skills in developing a reliable model for recommendations. This approach not only optimizes the model but also ensures an efficient learning process for better accuracy."
      ],
      "metadata": {
        "id": "txtht85QD0Zq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}