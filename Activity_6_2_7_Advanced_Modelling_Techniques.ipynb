{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheldonkemper/portfolio/blob/main/Activity_6_2_7_Advanced_Modelling_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "**First things first** - please go to 'File' and select 'Save a copy in Drive' so that you have your own version of this activity set up and ready to use.\n",
        "Remember to update the portfolio index link to your own work once completed!"
      ],
      "metadata": {
        "id": "cbtFdN5VQsms"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWsE_RrYod7r"
      },
      "source": [
        "\n",
        "# Activity: Exploring decision trees\n",
        "\n",
        "# Objective\n",
        "The objective of this activity is to combined all the concepts explore during this week, i.e. various tree-based models, pre- and post pruning, and how to interpret the models using SHAP values.\n",
        "\n",
        "# Instructions\n",
        "\n",
        "## 1. Data exploration\n",
        "- 1.1 Load the dataset and conduct basic explorations such as viewing the first few rows, describing the dataset to understand its structure, features, and target variable.\n",
        "- 1.2 Visualise the distribution of the target variable to check for imbalance.\n",
        "- 1.3 Correlation analysis to visualise relationships between the target variable and features using Seaborn or Matplotlib.\n",
        "\n",
        "## 2. Transformations\n",
        "- 2.1 Encode categorical variables using techniques like one-hot encoding or label encoding.\n",
        "- 2.2 Normalise or standardise numerical features if required. (Hint: tree-models don't need it)\n",
        "\n",
        "## 3. Compare basic models\n",
        "- 3.1 Compare basic decision tree models using pre-pruning (early stopping) vs post-pruning (CCP)\n",
        "- 3.2 Train basic tree models including decision tree, a bagging model (e.g., random forest), and a boosting model (AdaBoost, gradient boosting, XGBoost).\n",
        "- 3.3 For each model, train on the training set and check for overfitting using metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\n",
        "- 3.4 Compare the performance of these models and summarise the findings.\n",
        "\n",
        "## 4. Hyperparameter tuning: Pre-pruning and post-pruning\n",
        "- 4.1 Implement hyperparameter tuning for the decision tree model. Explore parameters such as `max_depth`, `min_samples_split`, and `max_features`, and regularisation paramteres such as `gamma`, `learning_rate`.\n",
        "- 4.2 Compare the performance of the tuned/pruned decision tree model against its baseline version.\n",
        "\n",
        "## 5. Interpretation using SHAP values\n",
        "- 5.1 Choose one of the models for interpretation (preferably a complex model like random forest or XGBoost).\n",
        "- 5.2 Visualise the SHAP values and interpret the results to understand the impact of different features on the model's predictions.\n",
        "\n",
        "#### Submission guidelines\n",
        "- Ensure your notebook is well-commented to explain your code and thought process.\n",
        "- Include visualisations to support your explorations and findings.\n",
        "- Summarise your insights and conclusions at the end of the Notebook.\n",
        "\n",
        "This activity is designed to provide a hands-on experience with decision trees and their ensemble counterparts, focusing on the entire machine learning workflow from data preprocessing to model interpretation. It is designed to allow flexibility to the user to implement what they deem appropriate, hence the results might vary from user to user, but are based on previous demonstration videos and data sets to those can be used as a benchmark.\n",
        "\n",
        "\n",
        "# Data set\n",
        "Use a classification dataset such as the UCI Machine Learning Repository's Bank Marketing data set. You can find more details about it on\n",
        "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing. This data set has been previously explored in the SHAP demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl-8nS20od73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "bank = pd.read_csv(\"https://raw.githubusercontent.com/fourthrevlxd/cam_dsb/main/C2_W6_Datasets/bank-additional-full-processed.csv\")\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "X = bank.drop('y', axis=1).copy()\n",
        "y = bank['y'].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3-WPVJwod78"
      },
      "source": [
        "## 1. Data exploration\n",
        "- 1.1 Load the data set and conduct basic explorations such as viewing the first few rows, describing the data set to understand its structure, features, and target variable.\n",
        "- 1.2 Visualise the distribution of the target variable to check for imbalance.\n",
        "- 1.3 Correlation analysis to visualise relationships between the target variable and features using Seaborn or Matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxSsyJY0od79"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdVvl7Iod7-"
      },
      "source": [
        "## 2. Transformations\n",
        "- 2.1 Encode categorical variables using techniques like one-hot encoding or label encoding.\n",
        "- 2.2 Normalise or standardise numerical features if required. (Hint: tree-models don't need it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AGARt0Tod8A"
      },
      "outputs": [],
      "source": [
        "std = StandardScaler()\n",
        "ohe = OneHotEncoder()\n",
        "lbe = LabelEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UM7Z8H5od8A"
      },
      "source": [
        "## 3. Compare basic models\n",
        "- 3.1 Train basic models including logistic regression, decision tree, a bagging model (e.g., random forest), and a boosting model (e.g., XGBoost).\n",
        "- 3.2 For each model, train on the training set and check for overfitting using metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\n",
        "- 3.3 Compare the performance of these models and summarise the findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx-zd7vGod8B"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(random_state=seed)\n",
        "dt = DecisionTreeClassifier(random_state=seed)\n",
        "rf = RandomForestClassifier(random_state=seed)\n",
        "ab = AdaBoostClassifier(random_state=seed)\n",
        "gb = GradientBoostingClassifier(random_state=seed)\n",
        "xgb = xgb.XGBClassifier(random_state=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O59GuBOMod8D"
      },
      "source": [
        "## 4. Hyperparameter tuning: Pre-pruning and post-pruning\n",
        "- 4.1 Implement hyperparameter tuning for the decision tree model. Explore parameters such as `max_depth`, `min_samples_split`, and `max_features`, and regularisation paramteres such as `gamma`, `learning_rate`.\n",
        "- 4.2 Compare the performance of the tuned/pruned decision tree model against its baseline version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3irwEScod8E"
      },
      "outputs": [],
      "source": [
        "models_param_grids = {\n",
        "    'Decision Tree': {\n",
        "        'model': ,\n",
        "        'param_grid': {\n",
        "\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': ,\n",
        "        'param_grid': {\n",
        "\n",
        "        }\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model': ,\n",
        "        'param_grid': {\n",
        "\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': ,\n",
        "        'param_grid': {\n",
        "\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "grid = GridSearchCV()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2KX0U6Uod8G"
      },
      "source": [
        "## 5. Interpretation using SHAP values\n",
        "- 5.1 Choose one of the models for interpretation (preferably a complex model like random forest or XGBoost).\n",
        "- 5.2 Visualise the SHAP values and interpret the results to understand the impact of different features on the model's predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjUQhZsood8H"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "# SHAP values\n",
        "shap_ex = shap.TreeExplainer(\"model\")  # replace with your trained model'\n",
        "vals = shap_ex(\"X_test\") # replace with your test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe-3h_UHod8I"
      },
      "outputs": [],
      "source": [
        "# Waterfall plot\n",
        "shap.plots.waterfall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGstBpiBod8J"
      },
      "outputs": [],
      "source": [
        "# Force plot\n",
        "shap.plots.force()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqCdufffod8J"
      },
      "outputs": [],
      "source": [
        "# SHAP scatter\n",
        "shap.plots.scatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TwGRh36od8K"
      },
      "outputs": [],
      "source": [
        "# Beeswarm plot\n",
        "shap.plots.beeswarm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBo-qZydod8L"
      },
      "outputs": [],
      "source": [
        "# Violin plot\n",
        "shap.plots.violin()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}